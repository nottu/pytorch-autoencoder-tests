{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas import read_fwf, DataFrame\n",
    "from tqdm   import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radioreader import *\n",
    "from methods import *\n",
    "from kittler import kittler_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Variational Auto Encoder\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, lt_dim=10):\n",
    "        super(VAE, self).__init__()\n",
    "        self.k = [1, 16, 32, 64, 128, 256]\n",
    "        encoder_layers = []\n",
    "        decoder_layers = []\n",
    "        \n",
    "        for i in range(len(self.k) - 1):\n",
    "            layer = nn.Conv2d(self.k[i], self.k[i+1], 3, 2, 1, 1)\n",
    "            encoder_layers.append(layer)\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(self.k) - 1, 0, -1):\n",
    "            layer = nn.ConvTranspose2d(self.k[i], self.k[i-1], 3, 2, 1, 1)\n",
    "            decoder_layers.append(layer)\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        self.decoder = nn.Sequential(*decoder_layers[:-1])\n",
    "        \n",
    "        self.fc_mu = nn.Linear(self.k[-1]*2*2, lt_dim)\n",
    "        self.fc_ep = nn.Linear(self.k[-1]*2*2, lt_dim)\n",
    "        \n",
    "        self.fc_dc = nn.Linear(lt_dim, self.k[-1]*2*2)\n",
    "    def encode(self, x):\n",
    "        encoded = self.encoder(x).view(-1, self.k[-1]*2*2)\n",
    "        return self.fc_mu(encoded), self.fc_ep(encoded)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, x):\n",
    "        x = F.relu(self.fc_dc(x)).view(-1, self.k[-1], 2, 2)\n",
    "        return torch.sigmoid(self.decoder(x))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, var = self.encode(x)\n",
    "        z = self.reparameterize(mu, var)\n",
    "        d = self.decode(z)\n",
    "        return d, mu, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  (fc_ep): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  (fc_dc): Linear(in_features=10, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = torch.load('b_vae_model', map_location='cpu')\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FITS_100(data.Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.data = images\n",
    "        self.data_len = len(self.data)\n",
    "        if(transform == None):\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(180),\n",
    "                transforms.CenterCrop(80),\n",
    "                transforms.Resize(64),\n",
    "                transforms.ToTensor()])\n",
    "        else : self.transform = transform\n",
    "\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        index = index % self.data_len\n",
    "        np_arr = self.data[index, :]\n",
    "        ## reshape np_arr to 28x28\n",
    "        np_arr = np_arr.reshape(128, 128)\n",
    "\n",
    "        ## convert to PIL-image\n",
    "        img = Image.fromarray((np_arr*255).astype('uint8'))\n",
    "\n",
    "        #apply the transformations and return tensors\n",
    "        return self.transform(img)\n",
    "    def __len__(self):\n",
    "        return self.data_len * 10\n",
    "    def __repr__(self) -> str:\n",
    "        return 'unLRG dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211c18963edb4da4b302d9174064b0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=101), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of images 101\n"
     ]
    }
   ],
   "source": [
    "directory = '101nvss_fit'\n",
    "ext = 'fit'\n",
    "names = glob.glob('{0}/*.{1}*'.format(directory, ext))\n",
    "images = []\n",
    "for n in tqdm(range(len(names))):\n",
    "    im = readImg(names[n], normalize=True, sz=64)\n",
    "    k = kittler_float(im, copy=False)\n",
    "    images.append( np.expand_dims(k, axis=0) )\n",
    "    del im\n",
    "    del k\n",
    "print(\"Number of images\", len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, m, s = vae(torch.Tensor(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
