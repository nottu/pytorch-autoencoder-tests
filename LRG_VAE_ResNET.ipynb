{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rg_dataset import LRG, UNLRG, UNLRG_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRG:\t1442/1442\n",
      "LRG:\t14245/14245\n"
     ]
    }
   ],
   "source": [
    "lrg_data_set = LRG(use_kittler=True)\n",
    "unlrg_data_set = UNLRG_C(use_kittler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_lrg   = DataLoader(lrg_data_set, batch_size=128, shuffle=False)\n",
    "data_loader_unlrg = DataLoader(unlrg_data_set, batch_size=128, shuffle=False)\n",
    "\n",
    "sample = iter(data_loader_lrg).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "\n",
    "def convT3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 Transpose convolution with padding\"\"\"\n",
    "    return nn.ConvTranspose2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, output_padding=(stride-1), groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "def convT1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 Transpose convolution\"\"\"\n",
    "    return nn.ConvTranspose2d(in_planes, out_planes, kernel_size=1, stride=stride, \n",
    "                              padding=0, output_padding=1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None, conv1=conv1x1, conv3=conv3x3, v=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.v = v\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        if(self.v > 1): out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = []\n",
    "n_filters=[1, 64, 128, 256, 512, 1024]\n",
    "for i in range(1, len(n_filters)):\n",
    "    f_i, f_o = n_filters[i - 1], n_filters[i]\n",
    "    downsample = nn.Sequential(conv1x1(f_i,  f_o, 2), nn.BatchNorm2d(f_o))\n",
    "    blocks.append(BasicBlock(f_i, f_o, stride=2, downsample=downsample))\n",
    "\n",
    "for i in range(len(n_filters)-1, 0, -1):\n",
    "    f_i, f_o = n_filters[i], n_filters[i-1]\n",
    "    downsample = nn.Sequential(convT1x1(f_i, f_o, 2), nn.BatchNorm2d(f_o))\n",
    "    blocks.append(BasicBlock(f_i, f_o, stride=2, downsample=downsample, conv1=convT1x1, conv3=convT3x3, v=f_o==1))\n",
    "\n",
    "bl = nn.Sequential(*blocks)\n",
    "# vae(sample[0].cuda())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNET_VAE(nn.Module):\n",
    "    def __init__(self, lt_dim=4):\n",
    "        super(ResNET_VAE, self).__init__()\n",
    "        self.k = [1, 16, 32, 64, 128, 256]\n",
    "        encoder_layers = []\n",
    "        for i in range(1, len(n_filters)):\n",
    "            f_i, f_o = self.k[i - 1], self.k[i]\n",
    "            downsample = nn.Sequential(conv1x1(f_i,  f_o, 2), nn.BatchNorm2d(f_o))\n",
    "            encoder_layers.append(BasicBlock(f_i, f_o, stride=2, downsample=downsample))\n",
    "            \n",
    "        decoder_layers = []\n",
    "        for i in range(len(n_filters)-1, 0, -1):\n",
    "            f_i, f_o = self.k[i], self.k[i-1]\n",
    "            downsample = nn.Sequential(convT1x1(f_i, f_o, 2), nn.BatchNorm2d(f_o))\n",
    "            decoder_layers.append(BasicBlock(f_i, f_o, stride=2, downsample=downsample, conv1=convT1x1, conv3=convT3x3, v=True))\n",
    "            \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(self.k[-1]*2*2, lt_dim)\n",
    "        self.fc_ep = nn.Linear(self.k[-1]*2*2, lt_dim)\n",
    "        \n",
    "        self.fc_dc = nn.Linear(lt_dim, self.k[-1]*2*2)\n",
    "    def encode(self, x):\n",
    "        encoded = self.encoder(x).view(-1, self.k[-1]*2*2)\n",
    "        return self.fc_mu(encoded), self.fc_ep(encoded)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, x):\n",
    "        x = F.relu(self.fc_dc(x)).view(-1, self.k[-1], 2, 2)\n",
    "        return torch.sigmoid(self.decoder(x))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, var = self.encode(x)\n",
    "        z = self.reparameterize(mu, var)\n",
    "        d = self.decode(z)\n",
    "        return d, mu, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class B_VAE_Loss:\n",
    "    def __init__(self, gamma, max_capacity, epochs):\n",
    "        self.gamma = gamma\n",
    "#         self.recon_ls = nn.MSELoss(reduction='sum')\n",
    "        self.recon_ls = nn.BCELoss(reduction='sum')\n",
    "        #self.recon_ls = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "        self.capacity = 0\n",
    "        self.delta = max_capacity / float(epochs)\n",
    "        self.max_capacity = max_capacity\n",
    "    def update(self):\n",
    "        self.capacity = min(self.max_capacity, self.capacity + self.delta)\n",
    "        return self.capacity\n",
    "    def __call__(self, res, img):\n",
    "        batch_sz = len(img)\n",
    "        x, mu, logvar = res\n",
    "        recon = self.recon_ls(x, img).div(batch_sz) #res -> x, mu, var\n",
    "        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()).div(batch_sz)\n",
    "        return self.capacity, recon, self.gamma * (kld - self.capacity).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "epochs = 1000\n",
    "vae = ResNET_VAE().to(device)\n",
    "beta_vae_loss = B_VAE_Loss(gamma=15, max_capacity=20, epochs=epochs)\n",
    "optimizer = Adam(vae.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, device, data_loader, optim, epoch, log_interval=5):\n",
    "    model.train()\n",
    "    s = ''\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        data = data.to(device)\n",
    "        target = Variable(target, requires_grad=False).to(device)\n",
    "        #Forward Pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        #############################################\n",
    "        ######### check against known class #########\n",
    "        #############################################\n",
    "        #########    Compact vs Extended    #########\n",
    "        extended = target > 1\n",
    "        extended = Variable(extended.float().to(device), requires_grad=False)\n",
    "        pred_ext = torch.sigmoid(output[1][:,0])\n",
    "        ext_loss = F.binary_cross_entropy(pred_ext, extended, reduction='sum').div(len(pred_ext))\n",
    "        #########       FRI vs FRII         #########\n",
    "        o = torch.sigmoid(output[1][:, 1])[target > 1]\n",
    "        c = target[target > 1]\n",
    "        o = o[c < 4]\n",
    "        c = c[c < 4]\n",
    "        c = Variable( (c == 3).float().to(device), requires_grad=False)\n",
    "        fr_loss = F.binary_cross_entropy(o, c, reduction='sum').div(len(c))\n",
    "        #########   Regular vs Irregular   #########\n",
    "#         o = torch.sigmoid(output[1][:, 2])[target > 1]\n",
    "#         c = target[target > 1]\n",
    "# #         o = o[c < 4]\n",
    "#         c = c < 4\n",
    "#         c = Variable( c.float().to(device), requires_grad=False)\n",
    "#         reg_loss = F.binary_cross_entropy(o, c, reduction='sum').div(len(c))\n",
    "        # BCE Loss\n",
    "        c, r_loss , g_loss = beta_vae_loss(output, data)\n",
    "        loss = r_loss + g_loss + 10 * (ext_loss + fr_loss)# + reg_loss)\n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        s = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:10.4f}\\tR_Loss: {:10.4f}\\tCapacity: {:10.4f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.item(), r_loss.item(), c)\n",
    "        if batch_idx % log_interval == 0:\n",
    "            sys.stdout.write('{}\\r'.format(s))\n",
    "            sys.stdout.flush()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [9408/14420 (99%)]\tLoss:  2433.3318\tR_Loss:  2418.6958\tCapacity:     0.0000\tTime 11.24s\n",
      "Train Epoch: 2 [9408/14420 (99%)]\tLoss:  2089.7239\tR_Loss:  2075.7327\tCapacity:     0.0200\tTime 10.98s\n",
      "Train Epoch: 3 [9408/14420 (99%)]\tLoss:  1806.4606\tR_Loss:  1793.1890\tCapacity:     0.0400\tTime 10.97s\n",
      "Train Epoch: 4 [9408/14420 (99%)]\tLoss:  1573.2854\tR_Loss:  1560.3340\tCapacity:     0.0600\tTime 10.97s\n",
      "Train Epoch: 5 [9408/14420 (99%)]\tLoss:  1382.2898\tR_Loss:  1369.2319\tCapacity:     0.0800\tTime 10.96s\n",
      "Train Epoch: 6 [9408/14420 (99%)]\tLoss:  1223.4957\tR_Loss:  1210.5829\tCapacity:     0.1000\tTime 10.94s\n",
      "Train Epoch: 7 [9408/14420 (99%)]\tLoss:  1092.3330\tR_Loss:  1080.7639\tCapacity:     0.1200\tTime 10.95s\n",
      "Train Epoch: 8 [9408/14420 (99%)]\tLoss:   983.8197\tR_Loss:   971.4859\tCapacity:     0.1400\tTime 10.95s\n",
      "Train Epoch: 9 [9408/14420 (99%)]\tLoss:   892.6757\tR_Loss:   880.4334\tCapacity:     0.1600\tTime 10.98s\n",
      "Train Epoch: 10 [9408/14420 (99%)]\tLoss:   816.3414\tR_Loss:   804.9968\tCapacity:     0.1800\tTime 10.96s\n",
      "Train Epoch: 11 [9408/14420 (99%)]\tLoss:   752.0209\tR_Loss:   740.0150\tCapacity:     0.2000\tTime 11.17s\n",
      "Train Epoch: 12 [9408/14420 (99%)]\tLoss:   697.9121\tR_Loss:   687.1545\tCapacity:     0.2200\tTime 11.06s\n",
      "Train Epoch: 13 [9408/14420 (99%)]\tLoss:   651.8030\tR_Loss:   640.4138\tCapacity:     0.2400\tTime 11.03s\n",
      "Train Epoch: 14 [9408/14420 (99%)]\tLoss:   611.2340\tR_Loss:   600.7548\tCapacity:     0.2600\tTime 11.00s\n",
      "Train Epoch: 15 [9408/14420 (99%)]\tLoss:   576.0955\tR_Loss:   565.5787\tCapacity:     0.2800\tTime 11.04s\n",
      "Train Epoch: 16 [9408/14420 (99%)]\tLoss:   530.7715\tR_Loss:   520.2304\tCapacity:     0.3000\tTime 11.05s\n",
      "Train Epoch: 17 [9408/14420 (99%)]\tLoss:   502.6084\tR_Loss:   491.3962\tCapacity:     0.3200\tTime 11.08s\n",
      "Train Epoch: 18 [9408/14420 (99%)]\tLoss:   480.6173\tR_Loss:   468.6292\tCapacity:     0.3400\tTime 11.07s\n",
      "Train Epoch: 19 [9408/14420 (99%)]\tLoss:   458.5945\tR_Loss:   447.5726\tCapacity:     0.3600\tTime 11.10s\n",
      "Train Epoch: 20 [9408/14420 (99%)]\tLoss:   438.4326\tR_Loss:   427.3710\tCapacity:     0.3800\tTime 11.16s\n",
      "Train Epoch: 21 [9408/14420 (99%)]\tLoss:   424.4539\tR_Loss:   412.7694\tCapacity:     0.4000\tTime 11.13s\n",
      "Train Epoch: 22 [9408/14420 (99%)]\tLoss:   412.0417\tR_Loss:   400.9650\tCapacity:     0.4200\tTime 11.04s\n",
      "Train Epoch: 23 [9408/14420 (99%)]\tLoss:   396.9212\tR_Loss:   383.8133\tCapacity:     0.4400\tTime 11.01s\n",
      "Train Epoch: 24 [9408/14420 (99%)]\tLoss:   384.0820\tR_Loss:   373.0178\tCapacity:     0.4600\tTime 11.03s\n",
      "Train Epoch: 25 [9408/14420 (99%)]\tLoss:   373.0833\tR_Loss:   362.2283\tCapacity:     0.4800\tTime 11.02s\n",
      "Train Epoch: 26 [9408/14420 (99%)]\tLoss:   363.9448\tR_Loss:   352.9080\tCapacity:     0.5000\tTime 11.05s\n",
      "Train Epoch: 27 [9408/14420 (99%)]\tLoss:   355.9052\tR_Loss:   344.5098\tCapacity:     0.5200\tTime 11.10s\n",
      "Train Epoch: 28 [9408/14420 (99%)]\tLoss:   348.8466\tR_Loss:   336.1607\tCapacity:     0.5400\tTime 11.11s\n",
      "Train Epoch: 29 [9408/14420 (99%)]\tLoss:   342.8001\tR_Loss:   330.2149\tCapacity:     0.5600\tTime 11.02s\n",
      "Train Epoch: 30 [9408/14420 (99%)]\tLoss:   335.4630\tR_Loss:   323.8388\tCapacity:     0.5800\tTime 11.05s\n",
      "Train Epoch: 31 [9408/14420 (99%)]\tLoss:   330.7037\tR_Loss:   319.5707\tCapacity:     0.6000\tTime 11.06s\n",
      "Train Epoch: 32 [9408/14420 (99%)]\tLoss:   320.9237\tR_Loss:   310.4037\tCapacity:     0.6200\tTime 11.03s\n",
      "Train Epoch: 33 [9408/14420 (99%)]\tLoss:   318.0907\tR_Loss:   306.0264\tCapacity:     0.6400\tTime 11.01s\n",
      "Train Epoch: 34 [9408/14420 (99%)]\tLoss:   312.5060\tR_Loss:   298.9211\tCapacity:     0.6600\tTime 11.05s\n",
      "Train Epoch: 35 [9408/14420 (99%)]\tLoss:   313.6000\tR_Loss:   301.9224\tCapacity:     0.6800\tTime 11.08s\n",
      "Train Epoch: 36 [9408/14420 (99%)]\tLoss:   309.8826\tR_Loss:   296.3539\tCapacity:     0.7000\tTime 11.14s\n",
      "Train Epoch: 37 [9408/14420 (99%)]\tLoss:   302.6522\tR_Loss:   289.7332\tCapacity:     0.7200\tTime 11.12s\n",
      "Train Epoch: 38 [9408/14420 (99%)]\tLoss:   299.0446\tR_Loss:   287.7789\tCapacity:     0.7400\tTime 11.05s\n",
      "Train Epoch: 39 [9408/14420 (99%)]\tLoss:   295.6797\tR_Loss:   280.3390\tCapacity:     0.7600\tTime 11.03s\n",
      "Train Epoch: 40 [9408/14420 (99%)]\tLoss:   297.2311\tR_Loss:   285.7798\tCapacity:     0.7800\tTime 11.05s\n",
      "Train Epoch: 41 [9408/14420 (99%)]\tLoss:   291.0901\tR_Loss:   278.9281\tCapacity:     0.8000\tTime 11.14s\n",
      "Train Epoch: 42 [9408/14420 (99%)]\tLoss:   286.4784\tR_Loss:   275.9226\tCapacity:     0.8200\tTime 11.15s\n",
      "Train Epoch: 43 [9408/14420 (99%)]\tLoss:   284.6515\tR_Loss:   271.6024\tCapacity:     0.8400\tTime 11.13s\n",
      "Train Epoch: 44 [9408/14420 (99%)]\tLoss:   282.6220\tR_Loss:   267.8178\tCapacity:     0.8600\tTime 11.08s\n",
      "Train Epoch: 45 [9408/14420 (99%)]\tLoss:   277.4658\tR_Loss:   264.5127\tCapacity:     0.8800\tTime 11.19s\n",
      "Train Epoch: 46 [9408/14420 (99%)]\tLoss:   280.7589\tR_Loss:   269.3731\tCapacity:     0.9000\tTime 11.03s\n",
      "Train Epoch: 47 [9408/14420 (99%)]\tLoss:   277.4868\tR_Loss:   266.5040\tCapacity:     0.9200\tTime 11.07s\n",
      "Train Epoch: 48 [9408/14420 (99%)]\tLoss:   279.6855\tR_Loss:   265.6825\tCapacity:     0.9400\tTime 11.09s\n",
      "Train Epoch: 49 [9408/14420 (99%)]\tLoss:   280.3174\tR_Loss:   268.6176\tCapacity:     0.9600\tTime 11.13s\n",
      "Train Epoch: 50 [9408/14420 (99%)]\tLoss:   272.3528\tR_Loss:   257.3891\tCapacity:     0.9800\tTime 11.07s\n",
      "Train Epoch: 51 [9408/14420 (99%)]\tLoss:   278.6149\tR_Loss:   264.2925\tCapacity:     1.0000\tTime 11.08s\n",
      "Train Epoch: 52 [9408/14420 (99%)]\tLoss:   273.2514\tR_Loss:   262.1481\tCapacity:     1.0200\tTime 11.14s\n",
      "Train Epoch: 53 [9408/14420 (99%)]\tLoss:   272.1066\tR_Loss:   261.1469\tCapacity:     1.0400\tTime 11.08s\n",
      "Train Epoch: 54 [9408/14420 (99%)]\tLoss:   271.6126\tR_Loss:   259.6133\tCapacity:     1.0600\tTime 11.05s\n",
      "Train Epoch: 55 [9408/14420 (99%)]\tLoss:   271.4130\tR_Loss:   259.1983\tCapacity:     1.0800\tTime 11.03s\n",
      "Train Epoch: 56 [9408/14420 (99%)]\tLoss:   267.3715\tR_Loss:   256.3682\tCapacity:     1.1000\tTime 11.14s\n",
      "Train Epoch: 57 [9408/14420 (99%)]\tLoss:   267.2735\tR_Loss:   255.6487\tCapacity:     1.1200\tTime 11.12s\n",
      "Train Epoch: 58 [9408/14420 (99%)]\tLoss:   271.4760\tR_Loss:   260.2611\tCapacity:     1.1400\tTime 11.03s\n",
      "Train Epoch: 59 [9408/14420 (99%)]\tLoss:   265.1599\tR_Loss:   254.1442\tCapacity:     1.1600\tTime 11.11s\n",
      "Train Epoch: 60 [9408/14420 (99%)]\tLoss:   266.2791\tR_Loss:   255.2393\tCapacity:     1.1800\tTime 11.08s\n",
      "Train Epoch: 61 [9408/14420 (99%)]\tLoss:   265.7545\tR_Loss:   251.0574\tCapacity:     1.2000\tTime 11.16s\n",
      "Train Epoch: 62 [9408/14420 (99%)]\tLoss:   259.8108\tR_Loss:   246.4778\tCapacity:     1.2200\tTime 11.09s\n",
      "Train Epoch: 63 [9408/14420 (99%)]\tLoss:   263.4341\tR_Loss:   252.2109\tCapacity:     1.2400\tTime 11.10s\n",
      "Train Epoch: 64 [9408/14420 (99%)]\tLoss:   266.0964\tR_Loss:   252.0605\tCapacity:     1.2600\tTime 11.11s\n",
      "Train Epoch: 65 [9408/14420 (99%)]\tLoss:   267.6825\tR_Loss:   255.6514\tCapacity:     1.2800\tTime 11.13s\n",
      "Train Epoch: 66 [9408/14420 (99%)]\tLoss:   266.1518\tR_Loss:   255.4885\tCapacity:     1.3000\tTime 11.12s\n",
      "Train Epoch: 67 [9408/14420 (99%)]\tLoss:   256.3399\tR_Loss:   244.9278\tCapacity:     1.3200\tTime 11.24s\n",
      "Train Epoch: 68 [9408/14420 (99%)]\tLoss:   261.4260\tR_Loss:   251.1310\tCapacity:     1.3400\tTime 11.11s\n",
      "Train Epoch: 69 [9408/14420 (99%)]\tLoss:   261.1133\tR_Loss:   249.7241\tCapacity:     1.3600\tTime 11.05s\n",
      "Train Epoch: 70 [9408/14420 (99%)]\tLoss:   257.7925\tR_Loss:   246.7824\tCapacity:     1.3800\tTime 11.09s\n",
      "Train Epoch: 71 [9408/14420 (99%)]\tLoss:   261.4588\tR_Loss:   249.0276\tCapacity:     1.4000\tTime 11.15s\n",
      "Train Epoch: 72 [9408/14420 (99%)]\tLoss:   257.8572\tR_Loss:   243.6793\tCapacity:     1.4200\tTime 11.08s\n",
      "Train Epoch: 73 [9408/14420 (99%)]\tLoss:   259.1523\tR_Loss:   247.3226\tCapacity:     1.4400\tTime 11.06s\n",
      "Train Epoch: 74 [9408/14420 (99%)]\tLoss:   260.9727\tR_Loss:   248.9497\tCapacity:     1.4600\tTime 11.08s\n",
      "Train Epoch: 75 [9408/14420 (99%)]\tLoss:   258.4104\tR_Loss:   246.8951\tCapacity:     1.4800\tTime 11.08s\n",
      "Train Epoch: 76 [9408/14420 (99%)]\tLoss:   259.8185\tR_Loss:   249.5443\tCapacity:     1.5000\tTime 11.07s\n",
      "Train Epoch: 77 [9408/14420 (99%)]\tLoss:   252.6110\tR_Loss:   241.0789\tCapacity:     1.5200\tTime 11.05s\n",
      "Train Epoch: 78 [9408/14420 (99%)]\tLoss:   257.9695\tR_Loss:   243.8953\tCapacity:     1.5400\tTime 11.06s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 79 [9408/14420 (99%)]\tLoss:   255.6728\tR_Loss:   245.4772\tCapacity:     1.5600\tTime 11.05s\n",
      "Train Epoch: 80 [9408/14420 (99%)]\tLoss:   262.3115\tR_Loss:   251.8286\tCapacity:     1.5800\tTime 11.03s\n",
      "Train Epoch: 81 [9408/14420 (99%)]\tLoss:   258.5494\tR_Loss:   247.1790\tCapacity:     1.6000\tTime 11.04s\n",
      "Train Epoch: 82 [9408/14420 (99%)]\tLoss:   255.4375\tR_Loss:   243.8467\tCapacity:     1.6200\tTime 11.02s\n",
      "Train Epoch: 83 [9408/14420 (99%)]\tLoss:   251.7890\tR_Loss:   236.8423\tCapacity:     1.6400\tTime 11.02s\n",
      "Train Epoch: 84 [9408/14420 (99%)]\tLoss:   247.9873\tR_Loss:   237.6035\tCapacity:     1.6600\tTime 11.01s\n",
      "Train Epoch: 85 [9408/14420 (99%)]\tLoss:   250.5504\tR_Loss:   237.2801\tCapacity:     1.6800\tTime 11.06s\n",
      "Train Epoch: 86 [9408/14420 (99%)]\tLoss:   255.6657\tR_Loss:   243.7920\tCapacity:     1.7000\tTime 11.04s\n",
      "Train Epoch: 87 [9408/14420 (99%)]\tLoss:   252.7150\tR_Loss:   241.6567\tCapacity:     1.7200\tTime 11.06s\n",
      "Train Epoch: 88 [9408/14420 (99%)]\tLoss:   249.2201\tR_Loss:   237.7347\tCapacity:     1.7400\tTime 11.05s\n",
      "Train Epoch: 89 [9408/14420 (99%)]\tLoss:   253.2499\tR_Loss:   241.7184\tCapacity:     1.7600\tTime 11.04s\n",
      "Train Epoch: 90 [9408/14420 (99%)]\tLoss:   253.4738\tR_Loss:   242.5766\tCapacity:     1.7800\tTime 11.03s\n",
      "Train Epoch: 91 [9408/14420 (99%)]\tLoss:   250.5144\tR_Loss:   237.2798\tCapacity:     1.8000\tTime 11.07s\n",
      "Train Epoch: 92 [9408/14420 (99%)]\tLoss:   250.5583\tR_Loss:   238.6805\tCapacity:     1.8200\tTime 11.08s\n",
      "Train Epoch: 93 [9408/14420 (99%)]\tLoss:   248.1802\tR_Loss:   234.5760\tCapacity:     1.8400\tTime 11.10s\n",
      "Train Epoch: 94 [9408/14420 (99%)]\tLoss:   247.3865\tR_Loss:   235.1232\tCapacity:     1.8600\tTime 11.08s\n",
      "Train Epoch: 95 [9408/14420 (99%)]\tLoss:   249.3854\tR_Loss:   239.7414\tCapacity:     1.8800\tTime 11.05s\n",
      "Train Epoch: 96 [9408/14420 (99%)]\tLoss:   247.0847\tR_Loss:   236.9114\tCapacity:     1.9000\tTime 11.09s\n",
      "Train Epoch: 97 [9408/14420 (99%)]\tLoss:   244.3199\tR_Loss:   233.8801\tCapacity:     1.9200\tTime 11.01s\n",
      "Train Epoch: 98 [9408/14420 (99%)]\tLoss:   252.0114\tR_Loss:   240.3359\tCapacity:     1.9400\tTime 11.05s\n",
      "Train Epoch: 99 [9408/14420 (99%)]\tLoss:   247.2708\tR_Loss:   236.6736\tCapacity:     1.9600\tTime 11.05s\n",
      "Train Epoch: 100 [9408/14420 (99%)]\tLoss:   244.7457\tR_Loss:   233.6917\tCapacity:     1.9800\tTime 11.05s\n",
      "Train Epoch: 101 [9408/14420 (99%)]\tLoss:   248.4901\tR_Loss:   237.0750\tCapacity:     2.0000\tTime 11.13s\n",
      "Train Epoch: 102 [9408/14420 (99%)]\tLoss:   242.4628\tR_Loss:   231.5744\tCapacity:     2.0200\tTime 11.10s\n",
      "Train Epoch: 103 [9408/14420 (99%)]\tLoss:   246.6004\tR_Loss:   236.5094\tCapacity:     2.0400\tTime 11.06s\n",
      "Train Epoch: 104 [9408/14420 (99%)]\tLoss:   245.6182\tR_Loss:   231.6212\tCapacity:     2.0600\tTime 11.04s\n",
      "Train Epoch: 105 [9408/14420 (99%)]\tLoss:   244.5573\tR_Loss:   233.9114\tCapacity:     2.0800\tTime 11.00s\n",
      "Train Epoch: 106 [9408/14420 (99%)]\tLoss:   243.6235\tR_Loss:   233.7991\tCapacity:     2.1000\tTime 11.08s\n",
      "Train Epoch: 107 [9408/14420 (99%)]\tLoss:   242.6947\tR_Loss:   233.0269\tCapacity:     2.1200\tTime 11.08s\n",
      "Train Epoch: 108 [9408/14420 (99%)]\tLoss:   252.9787\tR_Loss:   240.9561\tCapacity:     2.1400\tTime 11.07s\n",
      "Train Epoch: 109 [9408/14420 (99%)]\tLoss:   246.3309\tR_Loss:   236.6168\tCapacity:     2.1600\tTime 11.08s\n",
      "Train Epoch: 110 [9408/14420 (99%)]\tLoss:   243.6308\tR_Loss:   229.8958\tCapacity:     2.1800\tTime 11.05s\n",
      "Train Epoch: 111 [9408/14420 (99%)]\tLoss:   244.9233\tR_Loss:   231.5881\tCapacity:     2.2000\tTime 11.11s\n",
      "Train Epoch: 112 [9408/14420 (99%)]\tLoss:   240.5364\tR_Loss:   229.2684\tCapacity:     2.2200\tTime 11.09s\n",
      "Train Epoch: 113 [9408/14420 (99%)]\tLoss:   242.3683\tR_Loss:   230.4509\tCapacity:     2.2400\tTime 11.06s\n",
      "Train Epoch: 114 [9408/14420 (99%)]\tLoss:   244.9976\tR_Loss:   232.5922\tCapacity:     2.2600\tTime 11.05s\n",
      "Train Epoch: 115 [9408/14420 (99%)]\tLoss:   237.1821\tR_Loss:   227.3289\tCapacity:     2.2800\tTime 11.08s\n",
      "Train Epoch: 116 [9408/14420 (99%)]\tLoss:   238.8502\tR_Loss:   228.5404\tCapacity:     2.3000\tTime 11.07s\n",
      "Train Epoch: 117 [9408/14420 (99%)]\tLoss:   242.3324\tR_Loss:   232.0906\tCapacity:     2.3200\tTime 11.06s\n",
      "Train Epoch: 118 [9408/14420 (99%)]\tLoss:   236.5486\tR_Loss:   225.2679\tCapacity:     2.3400\tTime 11.31s\n",
      "Train Epoch: 119 [9408/14420 (99%)]\tLoss:   239.1617\tR_Loss:   228.9792\tCapacity:     2.3600\tTime 11.62s\n",
      "Train Epoch: 120 [9408/14420 (99%)]\tLoss:   238.6139\tR_Loss:   227.1006\tCapacity:     2.3800\tTime 11.64s\n",
      "Train Epoch: 121 [9408/14420 (99%)]\tLoss:   233.9375\tR_Loss:   224.8370\tCapacity:     2.4000\tTime 11.62s\n",
      "Train Epoch: 122 [9408/14420 (99%)]\tLoss:   242.3761\tR_Loss:   231.8923\tCapacity:     2.4200\tTime 11.15s\n",
      "Train Epoch: 123 [9408/14420 (99%)]\tLoss:   237.0107\tR_Loss:   226.3761\tCapacity:     2.4400\tTime 11.35s\n",
      "Train Epoch: 124 [9408/14420 (99%)]\tLoss:   236.6173\tR_Loss:   226.6037\tCapacity:     2.4600\tTime 11.81s\n",
      "Train Epoch: 125 [9408/14420 (99%)]\tLoss:   234.9878\tR_Loss:   224.5631\tCapacity:     2.4800\tTime 11.43s\n",
      "Train Epoch: 126 [9408/14420 (99%)]\tLoss:   240.4888\tR_Loss:   230.0457\tCapacity:     2.5000\tTime 11.69s\n",
      "Train Epoch: 127 [9408/14420 (99%)]\tLoss:   241.2091\tR_Loss:   230.9448\tCapacity:     2.5200\tTime 11.44s\n",
      "Train Epoch: 128 [9408/14420 (99%)]\tLoss:   243.7980\tR_Loss:   233.0344\tCapacity:     2.5400\tTime 11.47s\n",
      "Train Epoch: 129 [9408/14420 (99%)]\tLoss:   242.5443\tR_Loss:   232.6747\tCapacity:     2.5600\tTime 11.29s\n",
      "Train Epoch: 130 [9408/14420 (99%)]\tLoss:   237.6346\tR_Loss:   225.9557\tCapacity:     2.5800\tTime 11.54s\n",
      "Train Epoch: 131 [9408/14420 (99%)]\tLoss:   234.1954\tR_Loss:   224.5467\tCapacity:     2.6000\tTime 11.52s\n",
      "Train Epoch: 132 [9408/14420 (99%)]\tLoss:   233.9226\tR_Loss:   224.2929\tCapacity:     2.6200\tTime 11.43s\n",
      "Train Epoch: 133 [9408/14420 (99%)]\tLoss:   239.8947\tR_Loss:   229.3963\tCapacity:     2.6400\tTime 11.38s\n",
      "Train Epoch: 134 [9408/14420 (99%)]\tLoss:   234.7010\tR_Loss:   224.4031\tCapacity:     2.6600\tTime 11.55s\n",
      "Train Epoch: 135 [9408/14420 (99%)]\tLoss:   232.2346\tR_Loss:   222.4332\tCapacity:     2.6800\tTime 11.34s\n",
      "Train Epoch: 136 [9408/14420 (99%)]\tLoss:   231.3754\tR_Loss:   219.9741\tCapacity:     2.7000\tTime 11.28s\n",
      "Train Epoch: 137 [9408/14420 (99%)]\tLoss:   236.0126\tR_Loss:   226.5971\tCapacity:     2.7200\tTime 11.42s\n",
      "Train Epoch: 138 [9408/14420 (99%)]\tLoss:   232.0836\tR_Loss:   218.0002\tCapacity:     2.7400\tTime 11.59s\n",
      "Train Epoch: 139 [9408/14420 (99%)]\tLoss:   230.7924\tR_Loss:   220.3779\tCapacity:     2.7600\tTime 11.23s\n",
      "Train Epoch: 140 [9408/14420 (99%)]\tLoss:   233.1028\tR_Loss:   220.9833\tCapacity:     2.7800\tTime 11.63s\n",
      "Train Epoch: 141 [9408/14420 (99%)]\tLoss:   236.3654\tR_Loss:   226.0851\tCapacity:     2.8000\tTime 11.53s\n",
      "Train Epoch: 142 [9408/14420 (99%)]\tLoss:   226.2842\tR_Loss:   215.3960\tCapacity:     2.8200\tTime 11.31s\n",
      "Train Epoch: 143 [9408/14420 (99%)]\tLoss:   230.5249\tR_Loss:   220.6711\tCapacity:     2.8400\tTime 11.55s\n",
      "Train Epoch: 144 [9408/14420 (99%)]\tLoss:   227.3584\tR_Loss:   217.7590\tCapacity:     2.8600\tTime 11.44s\n",
      "Train Epoch: 145 [9408/14420 (99%)]\tLoss:   231.7966\tR_Loss:   222.6608\tCapacity:     2.8800\tTime 11.21s\n",
      "Train Epoch: 146 [9408/14420 (99%)]\tLoss:   231.2722\tR_Loss:   220.0771\tCapacity:     2.9000\tTime 11.20s\n",
      "Train Epoch: 147 [9408/14420 (99%)]\tLoss:   229.4884\tR_Loss:   217.2824\tCapacity:     2.9200\tTime 11.35s\n",
      "Train Epoch: 148 [9408/14420 (99%)]\tLoss:   227.0830\tR_Loss:   216.7113\tCapacity:     2.9400\tTime 11.48s\n",
      "Train Epoch: 149 [9408/14420 (99%)]\tLoss:   226.8627\tR_Loss:   215.8189\tCapacity:     2.9600\tTime 11.59s\n",
      "Train Epoch: 150 [9408/14420 (99%)]\tLoss:   230.2616\tR_Loss:   221.3100\tCapacity:     2.9800\tTime 11.30s\n",
      "Train Epoch: 151 [9408/14420 (99%)]\tLoss:   228.6671\tR_Loss:   219.3425\tCapacity:     3.0000\tTime 11.49s\n",
      "Train Epoch: 152 [9408/14420 (99%)]\tLoss:   228.9218\tR_Loss:   216.8030\tCapacity:     3.0200\tTime 11.55s\n",
      "Train Epoch: 153 [9408/14420 (99%)]\tLoss:   230.7083\tR_Loss:   217.4498\tCapacity:     3.0400\tTime 11.23s\n",
      "Train Epoch: 154 [9408/14420 (99%)]\tLoss:   224.8797\tR_Loss:   210.5906\tCapacity:     3.0600\tTime 11.45s\n",
      "Train Epoch: 155 [9408/14420 (99%)]\tLoss:   226.4806\tR_Loss:   217.5402\tCapacity:     3.0800\tTime 11.22s\n",
      "Train Epoch: 156 [9408/14420 (99%)]\tLoss:   225.5761\tR_Loss:   212.2922\tCapacity:     3.1000\tTime 11.32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 157 [9408/14420 (99%)]\tLoss:   226.1236\tR_Loss:   215.2441\tCapacity:     3.1200\tTime 11.49s\n",
      "Train Epoch: 158 [9408/14420 (99%)]\tLoss:   223.7874\tR_Loss:   214.9692\tCapacity:     3.1400\tTime 11.39s\n",
      "Train Epoch: 159 [9408/14420 (99%)]\tLoss:   229.1179\tR_Loss:   219.8717\tCapacity:     3.1600\tTime 11.48s\n",
      "Train Epoch: 160 [9408/14420 (99%)]\tLoss:   223.7124\tR_Loss:   212.9747\tCapacity:     3.1800\tTime 11.70s\n",
      "Train Epoch: 161 [9408/14420 (99%)]\tLoss:   224.6517\tR_Loss:   215.9106\tCapacity:     3.2000\tTime 11.37s\n",
      "Train Epoch: 162 [9408/14420 (99%)]\tLoss:   227.4722\tR_Loss:   217.4956\tCapacity:     3.2200\tTime 11.50s\n",
      "Train Epoch: 163 [9408/14420 (99%)]\tLoss:   222.4488\tR_Loss:   213.4596\tCapacity:     3.2400\tTime 11.55s\n",
      "Train Epoch: 164 [9408/14420 (99%)]\tLoss:   229.3111\tR_Loss:   217.9395\tCapacity:     3.2600\tTime 11.34s\n",
      "Train Epoch: 165 [9408/14420 (99%)]\tLoss:   224.6221\tR_Loss:   216.3613\tCapacity:     3.2800\tTime 11.40s\n",
      "Train Epoch: 166 [9408/14420 (99%)]\tLoss:   226.2639\tR_Loss:   215.7310\tCapacity:     3.3000\tTime 11.79s\n",
      "Train Epoch: 167 [9408/14420 (99%)]\tLoss:   223.4653\tR_Loss:   213.4008\tCapacity:     3.3200\tTime 11.55s\n",
      "Train Epoch: 168 [9408/14420 (99%)]\tLoss:   226.3810\tR_Loss:   216.0793\tCapacity:     3.3400\tTime 11.18s\n",
      "Train Epoch: 169 [9408/14420 (99%)]\tLoss:   227.0495\tR_Loss:   218.1269\tCapacity:     3.3600\tTime 11.25s\n",
      "Train Epoch: 170 [640/14420 (4%)]\tLoss:   215.9188\tR_Loss:   203.6633\tCapacity:     3.3800\r"
     ]
    }
   ],
   "source": [
    "beta_vae_loss = B_VAE_Loss(gamma=20, max_capacity=20, epochs=epochs)\n",
    "for epoch in range(1, epochs+1):\n",
    "    #LRG, forced params\n",
    "    start = time.time()\n",
    "    s = train_step(vae, 'cuda', data_loader_lrg, optimizer, epoch)\n",
    "    t = time.time() - start\n",
    "    sys.stdout.write('{0}\\tTime {1:.2f}s\\n'.format(s, t))\n",
    "    beta_vae_loss.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 50\n",
    "beta_vae_loss = B_VAE_Loss(gamma=10, max_capacity=20, epochs=eps)\n",
    "for epoch in range(1, eps+1):\n",
    "    #UN-LRG\n",
    "    start = time.time()\n",
    "    s = train_step(vae, 'cuda', data_loader_unlrg, optimizer, epoch)\n",
    "    t = time.time() - start\n",
    "    sys.stdout.write('{0}\\tTime {1:.2f}s\\n'.format(s, t))\n",
    "    beta_vae_loss.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1#6\n",
    "s, l = sample[0][a:a+1], sample[1][a:a+1]\n",
    "with torch.no_grad():\n",
    "    e = vae.encode(s.to(device))[0]\n",
    "    d = vae.decode(e)\n",
    "f, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "ax[0].imshow(s[0][0])\n",
    "ax[1].imshow(d.cpu()[0][0])\n",
    "ax[2].imshow(s[0][0] - d.cpu()[0][0], cmap='gray')\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "ax[2].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
