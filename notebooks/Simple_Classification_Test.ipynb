{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas import read_fwf, DataFrame\n",
    "from tqdm   import tqdm_notebook as tqdm\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "from skimage import exposure\n",
    "from skimage import measure\n",
    "from skimage.io import imsave\n",
    "from skimage.filters import gaussian as gaussian_filter\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage import filters\n",
    "from skimage.morphology import opening, closing, disk, binary_dilation, flood\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "from VAE.rg_dataset import LRG, BasicDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "aug = 10\n",
    "data_path = '../data/'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/lrg:\t1442/1442\n"
     ]
    }
   ],
   "source": [
    "lrg_data_set   = LRG(112, rd_sz=128, use_kittler=True, n_aug=aug, blur=False, \n",
    "                     catalog_dir=data_path + 'catalog/mrt-table3.txt', \n",
    "                     file_dir=data_path + 'lrg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = lrg_data_set.get_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BasicDataset(X_train, y_train, n_aug=15, sz=112) #\n",
    "test_dataset  = BasicDataset(X_test,  y_test,  n_aug=5,  sz=112)\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader  = data.DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAASX0lEQVR4nO3da4xdV3nG8f8zd4+v4wSbwXZiJ5gQJypNZEEIlFo4KeEijCoFmSqV20byFwqBUoFdqqJWQooKQvChVLK4WYACUYgaN6RA6gZFQBtsCFA7jnFInPgyvhA7vuDLzJx5+2GtQ8bTsWOfy5xjrecnWXv22ueceT32PPvd6+yztyICMytXR6sLMLPWcgiYFc4hYFY4h4BZ4RwCZoVzCJgVrmkhIOkOSTslPSNpXbO+j5nVR804T0BSJ/Br4HZgL7AF+EBEPNXwb2Zmdelq0uu+EXgmIp4FkPQtYBUwaQj0qDf6mN6kUswM4ARHfxsRr5o43qwQWADsGbe+F3jT+AdIWgusBeijnzdpZZNKMTOA/4wHnp9svFlzAppk7JzjjojYEBHLI2J5N71NKsPMXkmzQmAvsGjc+kJgf5O+l5nVoVkhsAVYKmmJpB5gNbCpSd/LzOrQlDmBiBiV9NfA94FO4CsRsb0Z38vM6tOsiUEi4hHgkWa9vpk1hs8YNCucQ8CscA4Bs8I5BMwK5xAwK5xDwKxwDgGzwjkEzArnEDArnEPArHAOAbPCOQTMCucQMCucQ8CscA4Bs8I5BMwK5xAwK5xDwKxwDgGzwjkEzArnEDArnEPArHAOAbPCOQTMCucQMCucQ8CscA4Bs8I5BMwK5xAwK5xDwKxwNYeApEWSHpO0Q9J2Sffk8bmSHpW0Ky8HGleumTVaPZ3AKPCxiLgeuAX4oKRlwDpgc0QsBTbndTNrUzWHQEQMRcTP89cngB3AAmAVsDE/bCPwvnqLNLPmacicgKTFwE3AE8D8iBiCFBTAvPM8Z62krZK2jnC2EWWYWQ3qDgFJM4DvAB+JiOMX+7yI2BARyyNieTe99ZZhZjWqKwQkdZMC4JsR8WAePihpMG8fBA7VV6KZNVM97w4I+DKwIyI+N27TJmBN/noN8FDt5ZlZs3XV8dy3AH8O/K+kX+SxvwPuBe6XdDfwAnBnfSWaWTPVHAIR8SNA59m8stbXNbOp5TMGzQrnEDArnEPArHAOAbPCOQTMCucQMCucQ8CscA4Bs8I5BMwK5xAwK5xDwKxwDgGzwjkEzArnEDArnEPArHAOAbPCOQTMCucQMCucQ8CscA4Bs8I5BMwK5xAwK5xDwKxwDgGzwjkEzArnEDArXD33IjSjo78/LV89D4A4egyAytGjLavJLo07AbPCuROwS6N0D9qOG64DYOTK1AmcXNADQP/BKwDoe/I5ACovHpnqCu0SuRMwK1zdnYCkTmArsC8i3iNpLvBtYDGwG3h/RPgA8TLXuex1AJxZOAuAY0u6Aaj0ps5Ao+lxY12pI6DjGgD6/idtqBw/PlWl2iVqRCdwD7Bj3Po6YHNELAU253Uza1N1dQKSFgLvBj4N/E0eXgWsyF9vBH4IfKKe72OtU+0Ajtw8F4Bjr037jeHZYwB0H0+dQPeJ6jPS+snB1CnErWnuoO/x7QCMnTrV9Jrt0tTbCXwe+DgwNm5sfkQMAeTlvMmeKGmtpK2Sto5wts4yzKxWNXcCkt4DHIqIn0lacanPj4gNwAaAWZobtdZhzVHtAF5cnmb7j12b9vBn5+eD/+6U+8M9ndVnpEXerSjvFoZnpfFpXX4jql3V8y/zFuC9kt4F9AGzJH0DOChpMCKGJA0ChxpRqJk1R80hEBHrgfUAuRP424i4S9JngDXAvXn5UAPqtCk2OpDe/z++OHcA887tAKqitwLA8Jx8/sBoagXyMLO3pfMEKidOYO2pGecJ3AvcLmkXcHteN7M21ZADtYj4IeldACLiRWBlI17XWmgsTdP05B34WD72H74ib88dgU6l8d6jaX/SfTJt7jqdnn/8+gEAZp++CoDR3S+kB4SngdqFzxg0K5ynbG1S3QdeAmD60LQ8kvYX0ZH2/JVpab3nWHWZn3cy7eGV5wSUd/gn/mA+AP179qXXGR1tWu12adwJmBXOnYBNavS55wEYyOtxyyAAqqT9RqUvvxswnLZ3/y7OWa92AJEexozn02TBWKXSxKqtFu4EzArnTsAuqNoRzB1Ne/Ajb10IwNnZuRPIO/ZqB9B1NncEI2k58zfp7YXY/kx6gN8VaDsOAbsoo3v2AtB7PB0WVHrO/a/Tfbo6ITj5L3+MDE9JnXbpfDhgVjh3AnZRlD8AVH3rr7rnj7wb6RxOJw9N350mAN0BXD7cCZgVzp2AXZSxN90IwPDM6klDabw6Adi/73R63C/zRaY8AXjZcCdgVjh3AnZRxrpzB9Cpc8ar678fdwdw2XEnYFY4dwJ2UaIj7/E1YUPejUS+KcnEzdb+3AmYFc6dgF1Q19WLADg9M32EOPL1RCs9aZ/fedZzAJc7dwJmhXMnYBc0sjBdT2x4RtpfjOaPEI/2Tzj67/BswOXKnYBZ4dwJ2EWp3nh0ZGZajuW5gZiVbzu2qA+AgSVXAy9/BNnanzsBs8K5E7ALy5ceH5meO4DuCZvznchPz037k86bXg3ArLzdlxhvf+4EzArnTsAuSvXTgmM91c8KpPHq9QU68hWFRvPcwfHcEcw8lT5dWDnoW1K2K3cCZoVzJ2AX1PXSKQCmHUk3ITmVzweo5HuSdOVLjVdvOtIxmpZdp/KNS32TkbbnTsCscO4E7IIqO3YBMG3+zLTek94OGB5LHUF3ahToHM5XG84NQO+LZwCI02emqlSrkTsBs8LV1QlImgN8CbgRCOCvgJ3At4HFwG7g/RFxtK4qrW10Vm8ycvrc8eqVhZTPKzh5VT8A0ztfC0DHr9LVh7U43bxEZ9ILjT67u4nV2sWotxP4AvC9iHg98AZgB7AO2BwRS4HNed3M2lTNnYCkWcDbgL8AiIhhYFjSKmBFfthG4IfAJ+op0lpP+Yy/yoQzBqtXGqpeX6A6J1C9E9HJRelthO4rbgDg2NXpBbryfQvmLBqg59nDwMt3ObKpVU8ncA1wGPiqpCclfUnSdGB+RAwB5OW8yZ4saa2krZK2jnC2jjLMrB71zAl0ATcDH4qIJyR9gUto/SNiA7ABYJbm+sTyNte9LX0qsGdOOsY/M6fznO3VjqB6vYHq1QYr+bMFh2/O1yNYkN81GEnrx5b2MfB0unrRlY/l13BHMKXq6QT2Ansj4om8/gApFA5KGgTIS58vatbGau4EIuKApD2SrouIncBK4Kn8Zw1wb14+1JBKraUqLx4BYMZjTwMQt10PwNmZ5+5HqtcZqPSm5cl0eQH6rn8JgL9f9kh6XL6F0Rd3r2B/z/w01pU6gnmPpue4I5ga9Z4s9CHgm5J6gGeBvyR1F/dLuht4Abizzu9hZk1UVwhExC+A5ZNsWlnP61r7qhw/DsDMx34NgN62FHh5jqB6fYFqBzB9WTpFZP31/wHAimn7AZjdkR4499qH+UzXHQA827kAgOhMHcH8R9JbDaP79jfnL2OAzxg0K54/O2A1qRxNe/gZP0pnAnbcci3w8nkAozPThQb+7NotAPxR3z4A5nXOOOd1/njaKc5ctRmAdUf/FIAzB9N1iV66NXUEsx9Pn0T0NQmaw52AWeHcCVhdqu8a9O8aAODEwnRumEbSeQJbXloMwDtmbAdgcMLzT42NcKSSuoOurtQ9jOazEqtnIZ55w1UAdP/AnUAzOASsoTpG0rLnaGoyt+xcAsA/d6TJv39Y8F0A5nem7f/+u6u4fyjNLZ84mj90lF+jMy/7dqdDj0pzSy+WDwfMCudOwBrjQPoQ0KzdcwAY687vFZKWP+5IE4f/xLsBeP2MAwD85MVrePr5dJDQfSA9tju9C8msp9IJRpVdzza39sK5EzArnDsBa4jqSUQ9W9JJRLM7XgdAKO3dFek84p+cuC4t51yTtldE18H0mP6hNBE4/7+PpW1Pp7cffeOS5nInYFY4dwLWUGMnTgDQ+6P0luDAWLqYyKlj6X2//gNpv3N2IN3AdKwLetKOn1f/OM0BxLbUTYQvVz4l3AmYFc6dgDXF2Jl08ZDux38JwGyl/U3HjOkAnLo1XZxkrEtM/02aTxjbvjM92XMAU8qdgFnh3AlYU008rq8cTZca7/3ult+PjU1pRTaROwGzwjkEzArnEDArnEPArHAOAbPCOQTMCucQMCucQ8CscA4Bs8I5BMwK5xAwK5xDwKxwDgGzwjkEzArnEDArXF0hIOmjkrZL2ibpPkl9kuZKelTSrrwcaFSxZtZ4NYeApAXAh4HlEXEj0AmsBtYBmyNiKbA5r5tZm6r3cKALmCapC+gH9gOrgI15+0bgfXV+DzNroppDICL2AZ8FXgCGgGMR8QNgfkQM5ccMAfMme76ktZK2Sto6wtlayzCzOtVzODBA2usvAV4DTJd018U+PyI2RMTyiFjeTW+tZZhZneo5HLgNeC4iDkfECPAgcCtwUNIgQF76pvJmbayeEHgBuEVSvyQBK4EdwCZgTX7MGuCh+ko0s2aq+ZLjEfGEpAeAnwOjwJPABmAGcL+ku0lBcWcjCjWz5qjrvgMR8SngUxOGz5K6AjO7DPiMQbPCOQTMCucQMCucQ8CscA4Bs8I5BMwK5xAwK5xDwKxwDgGzwjkEzArnEDArnEPArHAOAbPCOQTMCucQMCucQ8CscA4Bs8I5BMwK5xAwK5xDwKxwDgGzwjkEzArnEDArnEPArHAOAbPCOQTMCucQMCucQ8CscA4Bs8K9YghI+oqkQ5K2jRubK+lRSbvycmDctvWSnpG0U9I7mlW4mTXGxXQCXwPumDC2DtgcEUuBzXkdScuA1cAN+TlflNTZsGrNrOFeMQQi4nHgyIThVcDG/PVG4H3jxr8VEWcj4jngGeCNDarVzJqg1jmB+RExBJCX8/L4AmDPuMftzWP/j6S1krZK2jrC2RrLMLN6NXpiUJOMxWQPjIgNEbE8IpZ309vgMszsYtUaAgclDQLk5aE8vhdYNO5xC4H9tZdnZs1WawhsAtbkr9cAD40bXy2pV9ISYCnw0/pKNLNm6nqlB0i6D1gBXClpL/Ap4F7gfkl3Ay8AdwJExHZJ9wNPAaPAByOi0qTazawBXjEEIuID59m08jyP/zTw6XqKMrOp4zMGzQrnEDArnEPArHAOAbPCOQTMCucQMCucQ8CscA4Bs8I5BMwK5xAwK5xDwKxwDgGzwjkEzArnEDArnEPArHAOAbPCOQTMCqeISS8GPLVFSIeB3wG/bXUt53Elrq0Wru3SNbOuqyPiVRMH2yIEACRtjYjlra5jMq6tNq7t0rWiLh8OmBXOIWBWuHYKgQ2tLuACXFttXNulm/K62mZOwMxao506ATNrAYeAWeHaIgQk3SFpp6RnJK1rYR2LJD0maYek7ZLuyeNzJT0qaVdeDrSwxk5JT0p6uJ1qkzRH0gOSns4/vze3UW0fzf+e2yTdJ6mvVbVJ+oqkQ5K2jRs7by2S1uffi52S3tGMmloeApI6gX8B3gksAz4gaVmLyhkFPhYR1wO3AB/MtawDNkfEUmBzXm+Ve4Ad49bbpbYvAN+LiNcDbyDV2PLaJC0APgwsj4gbgU5gdQtr+xpwx4SxSWvJ//dWAzfk53wx/740VkS09A/wZuD749bXA+tbXVeu5SHgdmAnMJjHBoGdLapnYf5P8nbg4TzW8tqAWcBz5InmcePtUNsCYA8wl3TvzYeBP2llbcBiYNsr/Zwm/i4A3wfe3Oh6Wt4J8PI/UtXePNZSkhYDNwFPAPMjYgggL+e1qKzPAx8HxsaNtUNt1wCHga/mQ5UvSZreDrVFxD7gs6S7Zw8BxyLiB+1Q2zjnq2VKfjfaIQQ0yVhL37eUNAP4DvCRiDjeylqqJL0HOBQRP2t1LZPoAm4G/jUibiJ9DqSVh0y/l4+vVwFLgNcA0yXd1dqqLtqU/G60QwjsBRaNW18I7G9RLUjqJgXANyPiwTx8UNJg3j4IHGpBaW8B3itpN/At4O2SvtEmte0F9kbEE3n9AVIotENttwHPRcThiBgBHgRubZPaqs5Xy5T8brRDCGwBlkpaIqmHNBGyqRWFSBLwZWBHRHxu3KZNwJr89RrSXMGUioj1EbEwIhaTfkb/FRF3tUltB4A9kq7LQyuBp9qhNtJhwC2S+vO/70rSpGU71FZ1vlo2Aasl9UpaAiwFftrw7z7VEzXnmSh5F/Br4DfAJ1tYx1tJ7davgF/kP+8CriBNyO3Ky7kt/nmt4OWJwbaoDfhDYGv+2f0bMNBGtf0j8DSwDfg60Nuq2oD7SHMTI6Q9/d0XqgX4ZP692Am8sxk1+bRhs8K1w+GAmbWQQ8CscA4Bs8I5BMwK5xAwK5xDwKxwDgGzwv0f1Wjr//NnivoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = iter(test_dataloader).next()\n",
    "plt.imshow(sample[0].numpy()[10][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepClassifier(nn.Module):\n",
    "    ''' Somewhat inspired in VGG'''\n",
    "    def __init__(self, k=None, num_classes=6, lt_dim=8, batchnorm=True, in_channels=1, non_linearity=nn.ReLU,\n",
    "                 Conv2d=nn.Conv2d, MaxPool2d=nn.MaxPool2d, BatchNorm2d=nn.BatchNorm2d, n_angles=1):\n",
    "        super(DeepClassifier, self).__init__()\n",
    "        \n",
    "        self.non_linearity = non_linearity\n",
    "        self.Conv2d = Conv2d\n",
    "        self.MaxPool2d = MaxPool2d\n",
    "        self.BatchNorm2d = BatchNorm2d\n",
    "        self.n_angles = n_angles\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(k[-2] * 7 * 7, lt_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(lt_dim, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "        if k == None:\n",
    "            k = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "        layers = self.__make_layers(k, batchnorm)\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def __make_layers(self, k, batch_norm, in_channels=1):\n",
    "        layers = []\n",
    "        for v in k:\n",
    "            if v == 'M' :\n",
    "                layers += [self.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else :\n",
    "                conv2d = self.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, self.BatchNorm2d(v), self.non_linearity(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, self.non_linearity(inplace=True)]\n",
    "                in_channels = v\n",
    "#         for l in layers: print(l)\n",
    "        return layers\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def rotate_input(self, x, angle):\n",
    "        if angle == 0 or angle == 360 :\n",
    "            return x\n",
    "        bsz = x.shape[0]\n",
    "\n",
    "        r = torch.zeros([bsz, 2, 3], dtype=torch.float32, device='cuda')\n",
    "        r[:, 0, 0] =    torch.cos(torch.FloatTensor(bsz).fill_(angle))\n",
    "        r[:, 0, 1] =    torch.sin(torch.FloatTensor(bsz).fill_(angle))\n",
    "        r[:, 1, 0] = -1*torch.sin(torch.FloatTensor(bsz).fill_(angle))\n",
    "        r[:, 1, 1] =    torch.cos(torch.FloatTensor(bsz).fill_(angle))\n",
    "        \n",
    "        grid = F.affine_grid(r, x.size()).cuda()\n",
    "        x = F.grid_sample(x, grid, padding_mode='zeros')\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        ys = []\n",
    "        angle_steps = 360/self.n_angles\n",
    "        angle = 0\n",
    "        for a in range(self.n_angles):\n",
    "            x = self.rotate_input(x, angle)\n",
    "            y = self.features(x)\n",
    "            y = self.avgpool(y)\n",
    "            y = torch.flatten(y, 1)\n",
    "            y = self.classifier(y)\n",
    "#             y = F.softmax(y)\n",
    "            ys.append(y)\n",
    "            angle += angle_steps\n",
    "        y = torch.mean(torch.stack(ys), dim=0)\n",
    "        return y\n",
    "\n",
    "'''Common configs for VGG like networks'''\n",
    "cfgs = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, epoch, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    running_inputs=0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        running_inputs += len(labels.data)\n",
    "        \n",
    "        running_acc = (1.0 * running_corrects)/running_inputs\n",
    "        \n",
    "        s = 'Train Epoch: {:3d} ({:3.0f}%)\\tLoss:\\t{:4.4f}\\trLoss: {:4.2f}\\tTrain Acc: {:.4f}'\n",
    "        s = s.format(epoch,\n",
    "                100. * batch_idx / len(dataloader), loss.item(), running_loss, running_acc)\n",
    "        sys.stdout.write('{}\\r'.format(s))\n",
    "        sys.stdout.flush()\n",
    "    return running_loss, running_corrects, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, optimizer, epoch, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    running_inputs=0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        running_inputs += len(labels.data)\n",
    "        \n",
    "        running_acc = (1.0 * running_corrects)/running_inputs\n",
    "        \n",
    "#         s = 'Test Epoch: {:3d} ({:3.0f}%)\\tLoss:\\t{:4.4f}\\trLoss: {:4.2f}\\trPreds: {:.4f}'\n",
    "#         s = s.format(epoch,\n",
    "#                 100. * batch_idx / len(dataloader), loss.item(), running_loss, running_acc)\n",
    "#         sys.stdout.write('{}\\r'.format(s))\n",
    "#         sys.stdout.flush()\n",
    "\n",
    "    return running_loss, running_acc#, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:   1 (100%)\tLoss:\t1.2224\trLoss: 20896.52\tTrain Acc: 0.3767 Test Acc: 0.5818\tTime: 9.2091\n",
      "Train Epoch:   2 (100%)\tLoss:\t0.4940\trLoss: 15200.00\tTrain Acc: 0.5964 Test Acc: 0.6288\tTime: 9.1894\n",
      "Train Epoch:   3 (100%)\tLoss:\t1.6200\trLoss: 14197.32\tTrain Acc: 0.6327 Test Acc: 0.6436\tTime: 9.2926\n",
      "Train Epoch:   4 (100%)\tLoss:\t0.8517\trLoss: 13643.71\tTrain Acc: 0.6382 Test Acc: 0.6492\tTime: 9.6534\n",
      "Train Epoch:   5 (100%)\tLoss:\t0.7225\trLoss: 13174.08\tTrain Acc: 0.6466 Test Acc: 0.6521\tTime: 9.4057\n",
      "Train Epoch:   6 (100%)\tLoss:\t0.9078\trLoss: 12565.01\tTrain Acc: 0.6594 Test Acc: 0.6602\tTime: 9.2799\n",
      "Train Epoch:   7 (100%)\tLoss:\t0.9140\trLoss: 12351.45\tTrain Acc: 0.6665 Test Acc: 0.6394\tTime: 9.4187\n",
      "Train Epoch:   8 (100%)\tLoss:\t0.5850\trLoss: 11905.07\tTrain Acc: 0.6759 Test Acc: 0.6669\tTime: 9.1097\n",
      "Train Epoch:   9 (100%)\tLoss:\t0.5664\trLoss: 11493.60\tTrain Acc: 0.6924 Test Acc: 0.6754\tTime: 8.9225\n",
      "Train Epoch:  10 (100%)\tLoss:\t0.4867\trLoss: 11389.62\tTrain Acc: 0.6990 Test Acc: 0.6771\tTime: 8.9175\n",
      "Train Epoch:  11 (100%)\tLoss:\t0.7166\trLoss: 11168.86\tTrain Acc: 0.7047 Test Acc: 0.7042\tTime: 9.0015\n",
      "Train Epoch:  12 (100%)\tLoss:\t0.6284\trLoss: 10920.19\tTrain Acc: 0.7173 Test Acc: 0.7081\tTime: 8.8721\n",
      "Train Epoch:  13 (100%)\tLoss:\t0.6288\trLoss: 10732.54\tTrain Acc: 0.7218 Test Acc: 0.7114\tTime: 9.2055\n",
      "Train Epoch:  14 (100%)\tLoss:\t0.7070\trLoss: 10531.62\tTrain Acc: 0.7276 Test Acc: 0.6996\tTime: 9.0456\n",
      "Train Epoch:  15 (100%)\tLoss:\t0.8930\trLoss: 10330.22\tTrain Acc: 0.7347 Test Acc: 0.7097\tTime: 8.8320\n",
      "Train Epoch:  16 (100%)\tLoss:\t0.7552\trLoss: 10205.58\tTrain Acc: 0.7370 Test Acc: 0.7131\tTime: 8.8710\n",
      "Train Epoch:  17 (100%)\tLoss:\t0.4542\trLoss: 9999.85\tTrain Acc: 0.7432 Test Acc: 0.6924\tTime: 9.1666\n",
      "Train Epoch:  18 (100%)\tLoss:\t0.6985\trLoss: 9878.95\tTrain Acc: 0.7480 Test Acc: 0.7085\tTime: 9.2387\n",
      "Train Epoch:  19 (100%)\tLoss:\t0.6239\trLoss: 9695.31\tTrain Acc: 0.7494 Test Acc: 0.7186\tTime: 9.2791\n",
      "Train Epoch:  20 (100%)\tLoss:\t0.5924\trLoss: 9463.31\tTrain Acc: 0.7600 Test Acc: 0.7208\tTime: 9.2554\n",
      "Train Epoch:  21 (100%)\tLoss:\t0.4111\trLoss: 9343.59\tTrain Acc: 0.7613 Test Acc: 0.7229\tTime: 9.3318\n",
      "Train Epoch:  22 (100%)\tLoss:\t0.5882\trLoss: 9274.61\tTrain Acc: 0.7615 Test Acc: 0.7161\tTime: 9.3150\n",
      "Train Epoch:  23 (100%)\tLoss:\t0.5726\trLoss: 9103.00\tTrain Acc: 0.7659 Test Acc: 0.7322\tTime: 9.2125\n",
      "Train Epoch:  24 (100%)\tLoss:\t0.5570\trLoss: 9003.48\tTrain Acc: 0.7705 Test Acc: 0.7233\tTime: 9.2179\n",
      "Train Epoch:  25 (100%)\tLoss:\t0.8173\trLoss: 8829.40\tTrain Acc: 0.7737 Test Acc: 0.7415\tTime: 9.1695\n",
      "Train Epoch:  26 (100%)\tLoss:\t0.2219\trLoss: 8733.67\tTrain Acc: 0.7770 Test Acc: 0.7415\tTime: 9.1614\n",
      "Train Epoch:  27 (100%)\tLoss:\t0.6905\trLoss: 8548.59\tTrain Acc: 0.7787 Test Acc: 0.6907\tTime: 9.3775\n",
      "Train Epoch:  28 (100%)\tLoss:\t0.7435\trLoss: 8295.58\tTrain Acc: 0.7898 Test Acc: 0.7525\tTime: 9.3287\n",
      "Train Epoch:  29 (100%)\tLoss:\t0.2931\trLoss: 8241.72\tTrain Acc: 0.7916 Test Acc: 0.7538\tTime: 9.3409\n",
      "Train Epoch:  30 (100%)\tLoss:\t0.6303\trLoss: 8092.00\tTrain Acc: 0.7914 Test Acc: 0.7551\tTime: 9.2693\n",
      "Train Epoch:  31 (100%)\tLoss:\t0.6725\trLoss: 7597.01\tTrain Acc: 0.8106 Test Acc: 0.7631\tTime: 9.3608\n",
      "Train Epoch:  32 (100%)\tLoss:\t0.5433\trLoss: 7337.71\tTrain Acc: 0.8149 Test Acc: 0.7534\tTime: 9.3201\n",
      "Train Epoch:  33 (100%)\tLoss:\t0.4170\trLoss: 7381.39\tTrain Acc: 0.8111 Test Acc: 0.7517\tTime: 9.2576\n",
      "Train Epoch:  34 (100%)\tLoss:\t0.5673\trLoss: 7315.30\tTrain Acc: 0.8158 Test Acc: 0.7623\tTime: 9.2722\n",
      "Train Epoch:  35 (100%)\tLoss:\t0.6091\trLoss: 7315.75\tTrain Acc: 0.8146 Test Acc: 0.7360\tTime: 9.4493\n",
      "Train Epoch:  36 (100%)\tLoss:\t0.6102\trLoss: 7252.22\tTrain Acc: 0.8173 Test Acc: 0.7640\tTime: 9.2040\n",
      "Train Epoch:  37 (100%)\tLoss:\t0.4613\trLoss: 7198.37\tTrain Acc: 0.8169 Test Acc: 0.7458\tTime: 9.2170\n",
      "Train Epoch:  38 (100%)\tLoss:\t0.4846\trLoss: 7175.54\tTrain Acc: 0.8200 Test Acc: 0.7551\tTime: 9.2500\n",
      "Train Epoch:  39 (100%)\tLoss:\t0.2199\trLoss: 7130.17\tTrain Acc: 0.8180 Test Acc: 0.7631\tTime: 9.3516\n",
      "Train Epoch:  40 (100%)\tLoss:\t0.2306\trLoss: 7084.80\tTrain Acc: 0.8212 Test Acc: 0.7619\tTime: 9.3235\n",
      "Train Epoch:  41 (100%)\tLoss:\t0.5897\trLoss: 7078.34\tTrain Acc: 0.8199 Test Acc: 0.7470\tTime: 9.2976\n",
      "Train Epoch:  42 (100%)\tLoss:\t0.3081\trLoss: 7041.93\tTrain Acc: 0.8234 Test Acc: 0.7513\tTime: 9.2982\n",
      "Train Epoch:  43 (100%)\tLoss:\t1.0058\trLoss: 7039.60\tTrain Acc: 0.8220 Test Acc: 0.7525\tTime: 9.3703\n",
      "Train Epoch:  44 (100%)\tLoss:\t0.2933\trLoss: 7065.04\tTrain Acc: 0.8212 Test Acc: 0.7530\tTime: 9.3943\n",
      "Train Epoch:  45 (100%)\tLoss:\t0.6917\trLoss: 7000.19\tTrain Acc: 0.8217 Test Acc: 0.7610\tTime: 9.2520\n",
      "Train Epoch:  46 (100%)\tLoss:\t0.4651\trLoss: 6985.84\tTrain Acc: 0.8208 Test Acc: 0.7581\tTime: 9.1348\n",
      "Train Epoch:  47 (100%)\tLoss:\t0.5276\trLoss: 6889.06\tTrain Acc: 0.8265 Test Acc: 0.7508\tTime: 9.1644\n",
      "Train Epoch:  48 (100%)\tLoss:\t0.3092\trLoss: 6820.02\tTrain Acc: 0.8284 Test Acc: 0.7610\tTime: 9.2685\n",
      "Train Epoch:  49 (100%)\tLoss:\t0.3801\trLoss: 6844.33\tTrain Acc: 0.8286 Test Acc: 0.7521\tTime: 9.2939\n",
      "Train Epoch:  50 (100%)\tLoss:\t0.4235\trLoss: 6748.47\tTrain Acc: 0.8288 Test Acc: 0.7419\tTime: 9.3604\n",
      "Train Epoch:  51 (100%)\tLoss:\t0.6933\trLoss: 6784.00\tTrain Acc: 0.8308 Test Acc: 0.7487\tTime: 9.3292\n",
      "Train Epoch:  52 (100%)\tLoss:\t0.4710\trLoss: 6661.83\tTrain Acc: 0.8334 Test Acc: 0.7525\tTime: 9.2061\n",
      "Train Epoch:  53 (100%)\tLoss:\t0.5755\trLoss: 6759.48\tTrain Acc: 0.8313 Test Acc: 0.7445\tTime: 9.1589\n",
      "Train Epoch:  54 (100%)\tLoss:\t0.1202\trLoss: 6631.80\tTrain Acc: 0.8360 Test Acc: 0.7581\tTime: 9.2224\n",
      "Train Epoch:  55 (100%)\tLoss:\t0.3969\trLoss: 6659.77\tTrain Acc: 0.8355 Test Acc: 0.7517\tTime: 9.3591\n",
      "Train Epoch:  56 (100%)\tLoss:\t0.3046\trLoss: 6641.63\tTrain Acc: 0.8329 Test Acc: 0.7534\tTime: 9.2535\n",
      "Train Epoch:  57 (100%)\tLoss:\t0.6329\trLoss: 6545.57\tTrain Acc: 0.8355 Test Acc: 0.7623\tTime: 9.3377\n",
      "Train Epoch:  58 (100%)\tLoss:\t0.6996\trLoss: 6606.25\tTrain Acc: 0.8362 Test Acc: 0.7424\tTime: 9.3737\n",
      "Train Epoch:  59 (100%)\tLoss:\t0.8217\trLoss: 6708.36\tTrain Acc: 0.8325 Test Acc: 0.7602\tTime: 9.2693\n",
      "Train Epoch:  60 (100%)\tLoss:\t0.5554\trLoss: 6590.12\tTrain Acc: 0.8378 Test Acc: 0.7343\tTime: 9.3245\n",
      "Train Epoch:  61 (100%)\tLoss:\t0.4943\trLoss: 6524.24\tTrain Acc: 0.8381 Test Acc: 0.7470\tTime: 9.2799\n",
      "Train Epoch:  62 (100%)\tLoss:\t0.3448\trLoss: 6500.38\tTrain Acc: 0.8373 Test Acc: 0.7521\tTime: 9.2590\n",
      "Train Epoch:  63 (100%)\tLoss:\t0.6178\trLoss: 6425.55\tTrain Acc: 0.8367 Test Acc: 0.7534\tTime: 9.1142\n",
      "Train Epoch:  64 (100%)\tLoss:\t0.3208\trLoss: 6555.33\tTrain Acc: 0.8378 Test Acc: 0.7534\tTime: 9.2312\n",
      "Train Epoch:  65 (100%)\tLoss:\t0.3678\trLoss: 6584.25\tTrain Acc: 0.8376 Test Acc: 0.7500\tTime: 9.1671\n",
      "Train Epoch:  66 (100%)\tLoss:\t0.3371\trLoss: 6544.21\tTrain Acc: 0.8379 Test Acc: 0.7547\tTime: 9.4156\n",
      "Train Epoch:  67 (100%)\tLoss:\t0.4328\trLoss: 6414.10\tTrain Acc: 0.8414 Test Acc: 0.7415\tTime: 9.1542\n",
      "Train Epoch:  68 (100%)\tLoss:\t0.4389\trLoss: 6498.19\tTrain Acc: 0.8378 Test Acc: 0.7513\tTime: 9.1557\n",
      "Train Epoch:  69 (100%)\tLoss:\t0.2763\trLoss: 6439.01\tTrain Acc: 0.8408 Test Acc: 0.7551\tTime: 9.3402\n",
      "Train Epoch:  70 (100%)\tLoss:\t0.4557\trLoss: 6485.46\tTrain Acc: 0.8378 Test Acc: 0.7479\tTime: 9.2552\n",
      "Train Epoch:  71 (100%)\tLoss:\t0.4522\trLoss: 6409.26\tTrain Acc: 0.8419 Test Acc: 0.7470\tTime: 9.4195\n",
      "Train Epoch:  72 (100%)\tLoss:\t0.5332\trLoss: 6404.78\tTrain Acc: 0.8410 Test Acc: 0.7483\tTime: 9.3436\n",
      "Train Epoch:  73 (100%)\tLoss:\t0.1877\trLoss: 6449.13\tTrain Acc: 0.8403 Test Acc: 0.7517\tTime: 9.3750\n",
      "Train Epoch:  74 (100%)\tLoss:\t0.9025\trLoss: 6497.52\tTrain Acc: 0.8375 Test Acc: 0.7479\tTime: 9.3259\n",
      "Train Epoch:  75 (100%)\tLoss:\t0.6686\trLoss: 6324.58\tTrain Acc: 0.8416 Test Acc: 0.7534\tTime: 9.2963\n",
      "Train Epoch:  76 (100%)\tLoss:\t0.1151\trLoss: 6440.32\tTrain Acc: 0.8402 Test Acc: 0.7559\tTime: 9.2809\n",
      "Train Epoch:  77 (100%)\tLoss:\t0.4413\trLoss: 6557.09\tTrain Acc: 0.8370 Test Acc: 0.7593\tTime: 9.3829\n",
      "Train Epoch:  78 (100%)\tLoss:\t0.2465\trLoss: 6540.25\tTrain Acc: 0.8387 Test Acc: 0.7470\tTime: 9.2823\n",
      "Train Epoch:  79 (100%)\tLoss:\t0.2672\trLoss: 6365.98\tTrain Acc: 0.8401 Test Acc: 0.7564\tTime: 9.1375\n",
      "Train Epoch:  80 (100%)\tLoss:\t0.3834\trLoss: 6390.05\tTrain Acc: 0.8420 Test Acc: 0.7496\tTime: 9.1151\n",
      "Train Epoch:  81 (100%)\tLoss:\t0.4102\trLoss: 6485.59\tTrain Acc: 0.8366 Test Acc: 0.7576\tTime: 9.1793\n",
      "Train Epoch:  82 (100%)\tLoss:\t0.5571\trLoss: 6303.25\tTrain Acc: 0.8417 Test Acc: 0.7496\tTime: 9.1442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  83 (100%)\tLoss:\t0.7280\trLoss: 6481.95\tTrain Acc: 0.8369 Test Acc: 0.7572\tTime: 9.2361\n",
      "Train Epoch:  84 (100%)\tLoss:\t0.5628\trLoss: 6466.48\tTrain Acc: 0.8389 Test Acc: 0.7504\tTime: 9.1490\n",
      "Train Epoch:  85 (100%)\tLoss:\t0.4874\trLoss: 6551.61\tTrain Acc: 0.8334 Test Acc: 0.7517\tTime: 9.1979\n",
      "Train Epoch:  86 (100%)\tLoss:\t0.3220\trLoss: 6440.66\tTrain Acc: 0.8388 Test Acc: 0.7466\tTime: 9.1961\n",
      "Train Epoch:  87 (100%)\tLoss:\t0.5983\trLoss: 6516.90\tTrain Acc: 0.8380 Test Acc: 0.7492\tTime: 9.2000\n",
      "Train Epoch:  88 (100%)\tLoss:\t0.5329\trLoss: 6412.38\tTrain Acc: 0.8419 Test Acc: 0.7445\tTime: 8.9816\n",
      "Train Epoch:  89 (100%)\tLoss:\t0.2635\trLoss: 6327.62\tTrain Acc: 0.8419 Test Acc: 0.7508\tTime: 9.1024\n",
      "Train Epoch:  90 (100%)\tLoss:\t0.3336\trLoss: 6403.60\tTrain Acc: 0.8405 Test Acc: 0.7500\tTime: 9.0542\n",
      "Train Epoch:  91 (100%)\tLoss:\t0.2563\trLoss: 6325.83\tTrain Acc: 0.8443 Test Acc: 0.7564\tTime: 9.0104\n",
      "Train Epoch:  92 (100%)\tLoss:\t0.3726\trLoss: 6318.35\tTrain Acc: 0.8447 Test Acc: 0.7500\tTime: 9.1082\n",
      "Train Epoch:  93 (100%)\tLoss:\t0.3742\trLoss: 6406.03\tTrain Acc: 0.8385 Test Acc: 0.7534\tTime: 9.1667\n",
      "Train Epoch:  94 (100%)\tLoss:\t0.5839\trLoss: 6288.87\tTrain Acc: 0.8449 Test Acc: 0.7500\tTime: 9.2249\n",
      "Train Epoch:  95 (100%)\tLoss:\t0.3120\trLoss: 6384.29\tTrain Acc: 0.8405 Test Acc: 0.7547\tTime: 9.0158\n",
      "Train Epoch:  96 (100%)\tLoss:\t0.8153\trLoss: 6431.18\tTrain Acc: 0.8394 Test Acc: 0.7508\tTime: 9.2920\n",
      "Train Epoch:  97 (100%)\tLoss:\t0.7929\trLoss: 6397.38\tTrain Acc: 0.8404 Test Acc: 0.7411\tTime: 9.0683\n",
      "Train Epoch:  98 (100%)\tLoss:\t0.4437\trLoss: 6436.84\tTrain Acc: 0.8403 Test Acc: 0.7453\tTime: 9.2276\n",
      "Train Epoch:  99 (100%)\tLoss:\t0.3632\trLoss: 6305.56\tTrain Acc: 0.8442 Test Acc: 0.7462\tTime: 9.2686\n",
      "Train Epoch: 100 (100%)\tLoss:\t0.5903\trLoss: 6376.93\tTrain Acc: 0.8414 Test Acc: 0.7508\tTime: 9.2490\n"
     ]
    }
   ],
   "source": [
    "k = [16, 'M', 32, 'M', 64, 64, 'M', 128, 128, 'M']\n",
    "model = DeepClassifier(k=k, batchnorm=False, lt_dim=12).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=1E-5)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    start = time.time()\n",
    "    train_l, train_c, train_s = train(model, train_dataloader, optimizer, epoch, criterion)\n",
    "    test_l,  test_c           = test(model,  test_dataloader,  optimizer, epoch, criterion)\n",
    "    t = time.time() - start\n",
    "    print(train_s, 'Test Acc: {:.4f}\\tTime: {:.4f}'.format(test_c, t))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'vgg16_lt_12_norot_slim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:   1 (100%)\tLoss:\t1.1146\trLoss: 18420.56\tTrain Acc: 0.4613 Test Acc: 0.6415\tTime: 40.1315\n",
      "Train Epoch:   2 (100%)\tLoss:\t0.9460\trLoss: 14230.20\tTrain Acc: 0.6266 Test Acc: 0.5695\tTime: 40.3830\n",
      "Train Epoch:   3 (100%)\tLoss:\t0.7905\trLoss: 13208.31\tTrain Acc: 0.6406 Test Acc: 0.6347\tTime: 40.5623\n",
      "Train Epoch:   4 (100%)\tLoss:\t0.7198\trLoss: 12184.03\tTrain Acc: 0.6690 Test Acc: 0.7161\tTime: 40.3843\n",
      "Train Epoch:   5 (100%)\tLoss:\t1.2461\trLoss: 11606.61\tTrain Acc: 0.6936 Test Acc: 0.7144\tTime: 40.5554\n",
      "Train Epoch:   6 (100%)\tLoss:\t1.1252\trLoss: 10938.09\tTrain Acc: 0.7178 Test Acc: 0.7305\tTime: 40.6335\n",
      "Train Epoch:   7 (100%)\tLoss:\t0.7188\trLoss: 10146.23\tTrain Acc: 0.7433 Test Acc: 0.7458\tTime: 40.5927\n",
      "Train Epoch:   8 (100%)\tLoss:\t0.9503\trLoss: 9725.38\tTrain Acc: 0.7551 Test Acc: 0.7326\tTime: 40.6760\n",
      "Train Epoch:   9 (100%)\tLoss:\t0.2827\trLoss: 8983.03\tTrain Acc: 0.7760 Test Acc: 0.7415\tTime: 40.8584\n",
      "Train Epoch:  10 (100%)\tLoss:\t0.7002\trLoss: 8405.91\tTrain Acc: 0.7900 Test Acc: 0.7606\tTime: 40.7824\n",
      "Train Epoch:  11 (100%)\tLoss:\t0.5626\trLoss: 7793.21\tTrain Acc: 0.8058 Test Acc: 0.7949\tTime: 40.5495\n",
      "Train Epoch:  12 (100%)\tLoss:\t0.1587\trLoss: 7520.59\tTrain Acc: 0.8148 Test Acc: 0.7907\tTime: 40.6665\n",
      "Train Epoch:  13 (100%)\tLoss:\t0.2972\trLoss: 6979.88\tTrain Acc: 0.8291 Test Acc: 0.7928\tTime: 40.7018\n",
      "Train Epoch:  14 (100%)\tLoss:\t0.2414\trLoss: 6431.86\tTrain Acc: 0.8431 Test Acc: 0.7869\tTime: 40.7318\n",
      "Train Epoch:  15 (100%)\tLoss:\t0.3743\trLoss: 6066.06\tTrain Acc: 0.8518 Test Acc: 0.7809\tTime: 40.8320\n",
      "Train Epoch:  16 (100%)\tLoss:\t0.4717\trLoss: 5649.03\tTrain Acc: 0.8637 Test Acc: 0.7958\tTime: 40.8124\n",
      "Train Epoch:  17 (100%)\tLoss:\t0.4986\trLoss: 5353.52\tTrain Acc: 0.8707 Test Acc: 0.7818\tTime: 40.8760\n",
      "Train Epoch:  18 (100%)\tLoss:\t0.2071\trLoss: 5205.65\tTrain Acc: 0.8727 Test Acc: 0.7847\tTime: 41.0257\n",
      "Train Epoch:  19 (100%)\tLoss:\t0.2373\trLoss: 4700.57\tTrain Acc: 0.8862 Test Acc: 0.7873\tTime: 40.7765\n",
      "Train Epoch:  20 (100%)\tLoss:\t0.1965\trLoss: 4394.77\tTrain Acc: 0.8931 Test Acc: 0.7941\tTime: 40.8792\n",
      "Train Epoch:  21 (100%)\tLoss:\t0.4001\trLoss: 4233.75\tTrain Acc: 0.8954 Test Acc: 0.7733\tTime: 40.9003\n",
      "Train Epoch:  22 (100%)\tLoss:\t0.1101\trLoss: 3879.83\tTrain Acc: 0.9041 Test Acc: 0.7619\tTime: 40.8044\n",
      "Train Epoch:  23 (100%)\tLoss:\t0.5359\trLoss: 3637.61\tTrain Acc: 0.9099 Test Acc: 0.7669\tTime: 40.9511\n",
      "Train Epoch:  24 (100%)\tLoss:\t0.0804\trLoss: 3388.37\tTrain Acc: 0.9165 Test Acc: 0.7907\tTime: 40.9764\n",
      "Train Epoch:  25 (100%)\tLoss:\t0.2321\trLoss: 3348.37\tTrain Acc: 0.9181 Test Acc: 0.7725\tTime: 40.9592\n",
      "Train Epoch:  26 (100%)\tLoss:\t0.5031\trLoss: 3154.69\tTrain Acc: 0.9243 Test Acc: 0.7780\tTime: 41.0565\n",
      "Train Epoch:  27 (100%)\tLoss:\t0.1901\trLoss: 2896.24\tTrain Acc: 0.9296 Test Acc: 0.7538\tTime: 41.0304\n",
      "Train Epoch:  28 (100%)\tLoss:\t0.0541\trLoss: 2979.69\tTrain Acc: 0.9266 Test Acc: 0.7733\tTime: 40.8497\n",
      "Train Epoch:  29 (100%)\tLoss:\t0.1852\trLoss: 2641.57\tTrain Acc: 0.9333 Test Acc: 0.7301\tTime: 40.9014\n",
      "Train Epoch:  30 (100%)\tLoss:\t0.2437\trLoss: 2733.92\tTrain Acc: 0.9339 Test Acc: 0.7602\tTime: 40.7609\n",
      "Train Epoch:  31 (100%)\tLoss:\t0.1358\trLoss: 2517.92\tTrain Acc: 0.9383 Test Acc: 0.8110\tTime: 41.1560\n",
      "Train Epoch:  32 (100%)\tLoss:\t0.1207\trLoss: 2492.63\tTrain Acc: 0.9383 Test Acc: 0.7864\tTime: 41.0823\n",
      "Train Epoch:  33 (100%)\tLoss:\t0.0518\trLoss: 2402.88\tTrain Acc: 0.9418 Test Acc: 0.7852\tTime: 40.6368\n",
      "Train Epoch:  34 (100%)\tLoss:\t0.2562\trLoss: 2463.32\tTrain Acc: 0.9411 Test Acc: 0.7801\tTime: 40.8035\n",
      "Train Epoch:  35 (100%)\tLoss:\t0.1914\trLoss: 2444.92\tTrain Acc: 0.9397 Test Acc: 0.7648\tTime: 40.7769\n",
      "Train Epoch:  36 (100%)\tLoss:\t0.0919\trLoss: 2078.62\tTrain Acc: 0.9489 Test Acc: 0.7606\tTime: 40.8489\n",
      "Train Epoch:  37 (100%)\tLoss:\t0.0429\trLoss: 2478.80\tTrain Acc: 0.9420 Test Acc: 0.7513\tTime: 40.9274\n",
      "Train Epoch:  38 (100%)\tLoss:\t0.3975\trLoss: 2011.03\tTrain Acc: 0.9527 Test Acc: 0.7021\tTime: 40.8816\n",
      "Train Epoch:  39 (100%)\tLoss:\t0.0142\trLoss: 2010.66\tTrain Acc: 0.9544 Test Acc: 0.7551\tTime: 40.7777\n",
      "Train Epoch:  40 (100%)\tLoss:\t0.0165\trLoss: 1858.81\tTrain Acc: 0.9551 Test Acc: 0.7538\tTime: 40.8242\n",
      "Train Epoch:  41 (100%)\tLoss:\t0.1673\trLoss: 1820.96\tTrain Acc: 0.9597 Test Acc: 0.7801\tTime: 40.8002\n",
      "Train Epoch:  42 (100%)\tLoss:\t0.0467\trLoss: 1979.15\tTrain Acc: 0.9546 Test Acc: 0.7614\tTime: 40.8698\n",
      "Train Epoch:  43 (100%)\tLoss:\t0.1751\trLoss: 1911.77\tTrain Acc: 0.9580 Test Acc: 0.7640\tTime: 40.9452\n",
      "Train Epoch:  44 (100%)\tLoss:\t0.0071\trLoss: 1816.22\tTrain Acc: 0.9591 Test Acc: 0.7280\tTime: 40.8096\n",
      "Train Epoch:  45 (100%)\tLoss:\t0.0858\trLoss: 1767.95\tTrain Acc: 0.9615 Test Acc: 0.7534\tTime: 40.6675\n",
      "Train Epoch:  46 (100%)\tLoss:\t0.2208\trLoss: 1825.12\tTrain Acc: 0.9588 Test Acc: 0.7716\tTime: 41.0032\n",
      "Train Epoch:  47 (100%)\tLoss:\t0.0322\trLoss: 1726.47\tTrain Acc: 0.9609 Test Acc: 0.7538\tTime: 40.9723\n",
      "Train Epoch:  48 (100%)\tLoss:\t0.0324\trLoss: 1559.10\tTrain Acc: 0.9654 Test Acc: 0.7415\tTime: 40.8669\n",
      "Train Epoch:  49 (100%)\tLoss:\t0.0501\trLoss: 1720.74\tTrain Acc: 0.9611 Test Acc: 0.7695\tTime: 40.9708\n",
      "Train Epoch:  50 (100%)\tLoss:\t0.0125\trLoss: 1510.87\tTrain Acc: 0.9656 Test Acc: 0.7496\tTime: 40.7518\n",
      "Train Epoch:  51 (100%)\tLoss:\t0.0373\trLoss: 1340.15\tTrain Acc: 0.9688 Test Acc: 0.7775\tTime: 40.7595\n",
      "Train Epoch:  52 (100%)\tLoss:\t0.0045\trLoss: 1626.07\tTrain Acc: 0.9636 Test Acc: 0.7568\tTime: 40.6327\n",
      "Train Epoch:  53 (100%)\tLoss:\t0.0084\trLoss: 1565.20\tTrain Acc: 0.9651 Test Acc: 0.7818\tTime: 40.7251\n",
      "Train Epoch:  54 (100%)\tLoss:\t0.0472\trLoss: 1677.35\tTrain Acc: 0.9624 Test Acc: 0.7737\tTime: 40.7880\n",
      "Train Epoch:  55 (100%)\tLoss:\t0.0173\trLoss: 1407.54\tTrain Acc: 0.9696 Test Acc: 0.7699\tTime: 40.9103\n",
      "Train Epoch:  56 (100%)\tLoss:\t0.2017\trLoss: 1400.69\tTrain Acc: 0.9689 Test Acc: 0.7441\tTime: 40.8448\n",
      "Train Epoch:  57 (100%)\tLoss:\t0.1807\trLoss: 1464.81\tTrain Acc: 0.9678 Test Acc: 0.7496\tTime: 40.7996\n",
      "Train Epoch:  58 (100%)\tLoss:\t0.1368\trLoss: 1311.60\tTrain Acc: 0.9696 Test Acc: 0.7441\tTime: 40.8414\n",
      "Train Epoch:  59 (100%)\tLoss:\t0.0050\trLoss: 1521.56\tTrain Acc: 0.9656 Test Acc: 0.7386\tTime: 40.7731\n",
      "Train Epoch:  60 (100%)\tLoss:\t0.0481\trLoss: 1466.74\tTrain Acc: 0.9650 Test Acc: 0.7699\tTime: 40.8075\n",
      "Train Epoch:  61 (100%)\tLoss:\t0.0103\trLoss: 1400.73\tTrain Acc: 0.9678 Test Acc: 0.7398\tTime: 40.8708\n",
      "Train Epoch:  62 (100%)\tLoss:\t0.0828\trLoss: 1331.82\tTrain Acc: 0.9693 Test Acc: 0.7339\tTime: 40.7391\n",
      "Train Epoch:  63 (100%)\tLoss:\t0.0470\trLoss: 1139.97\tTrain Acc: 0.9753 Test Acc: 0.7716\tTime: 40.6681\n",
      "Train Epoch:  64 (100%)\tLoss:\t0.0687\trLoss: 1554.19\tTrain Acc: 0.9647 Test Acc: 0.7318\tTime: 40.6499\n",
      "Train Epoch:  65 (100%)\tLoss:\t0.0104\trLoss: 1321.50\tTrain Acc: 0.9693 Test Acc: 0.7538\tTime: 40.5930\n",
      "Train Epoch:  66 (100%)\tLoss:\t0.0140\trLoss: 1353.89\tTrain Acc: 0.9696 Test Acc: 0.7627\tTime: 40.6499\n",
      "Train Epoch:  67 (100%)\tLoss:\t0.2367\trLoss: 1303.66\tTrain Acc: 0.9702 Test Acc: 0.7165\tTime: 40.7140\n",
      "Train Epoch:  68 (100%)\tLoss:\t0.1859\trLoss: 1247.49\tTrain Acc: 0.9723 Test Acc: 0.7394\tTime: 40.7775\n",
      "Train Epoch:  69 (100%)\tLoss:\t0.0036\trLoss: 1136.12\tTrain Acc: 0.9739 Test Acc: 0.7581\tTime: 40.5891\n",
      "Train Epoch:  70 (100%)\tLoss:\t0.0010\trLoss: 1303.19\tTrain Acc: 0.9689 Test Acc: 0.7593\tTime: 40.6382\n",
      "Train Epoch:  71 (100%)\tLoss:\t0.1301\trLoss: 1271.47\tTrain Acc: 0.9705 Test Acc: 0.7653\tTime: 40.7559\n",
      "Train Epoch:  72 (100%)\tLoss:\t0.0345\trLoss: 1103.68\tTrain Acc: 0.9760 Test Acc: 0.7771\tTime: 40.7883\n",
      "Train Epoch:  73 (100%)\tLoss:\t0.0046\trLoss: 1126.33\tTrain Acc: 0.9746 Test Acc: 0.7373\tTime: 40.4700\n",
      "Train Epoch:  74 (100%)\tLoss:\t0.0166\trLoss: 1159.59\tTrain Acc: 0.9734 Test Acc: 0.7877\tTime: 40.6936\n",
      "Train Epoch:  75 (100%)\tLoss:\t0.0939\trLoss: 1141.52\tTrain Acc: 0.9738 Test Acc: 0.7758\tTime: 40.6877\n",
      "Train Epoch:  76 (100%)\tLoss:\t0.0400\trLoss: 1310.62\tTrain Acc: 0.9714 Test Acc: 0.7631\tTime: 40.7751\n",
      "Train Epoch:  77 (100%)\tLoss:\t0.0589\trLoss: 1325.57\tTrain Acc: 0.9683 Test Acc: 0.7831\tTime: 40.8507\n",
      "Train Epoch:  78 (100%)\tLoss:\t0.1950\trLoss: 1039.08\tTrain Acc: 0.9758 Test Acc: 0.7716\tTime: 40.7102\n",
      "Train Epoch:  79 (100%)\tLoss:\t0.0053\trLoss: 968.73\tTrain Acc: 0.9777 Test Acc: 0.7606\tTime: 40.6366\n",
      "Train Epoch:  80 (100%)\tLoss:\t0.1038\trLoss: 1139.22\tTrain Acc: 0.9750 Test Acc: 0.7564\tTime: 40.8014\n",
      "Train Epoch:  81 (100%)\tLoss:\t0.1483\trLoss: 1527.55\tTrain Acc: 0.9686 Test Acc: 0.7703\tTime: 40.8427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  82 (100%)\tLoss:\t0.0969\trLoss: 1024.44\tTrain Acc: 0.9767 Test Acc: 0.7432\tTime: 40.2589\n",
      "Train Epoch:  83 (100%)\tLoss:\t0.0008\trLoss: 1020.87\tTrain Acc: 0.9753 Test Acc: 0.7441\tTime: 40.3454\n",
      "Train Epoch:  84 (100%)\tLoss:\t0.1210\trLoss: 1103.44\tTrain Acc: 0.9742 Test Acc: 0.7699\tTime: 40.3215\n",
      "Train Epoch:  85 (100%)\tLoss:\t0.1051\trLoss: 1011.99\tTrain Acc: 0.9767 Test Acc: 0.7496\tTime: 40.2693\n",
      "Train Epoch:  86 (100%)\tLoss:\t0.0614\trLoss: 852.55\tTrain Acc: 0.9797 Test Acc: 0.7614\tTime: 40.3248\n",
      "Train Epoch:  87 (100%)\tLoss:\t0.0934\trLoss: 994.30\tTrain Acc: 0.9751 Test Acc: 0.7814\tTime: 40.7054\n",
      "Train Epoch:  88 (100%)\tLoss:\t0.0005\trLoss: 845.15\tTrain Acc: 0.9792 Test Acc: 0.7275\tTime: 40.6027\n",
      "Train Epoch:  89 (100%)\tLoss:\t0.1502\trLoss: 968.35\tTrain Acc: 0.9781 Test Acc: 0.7898\tTime: 40.5615\n",
      "Train Epoch:  90 (100%)\tLoss:\t0.3703\trLoss: 1061.91\tTrain Acc: 0.9764 Test Acc: 0.7843\tTime: 40.2928\n",
      "Train Epoch:  91 (100%)\tLoss:\t0.0512\trLoss: 998.88\tTrain Acc: 0.9775 Test Acc: 0.7742\tTime: 40.2446\n",
      "Train Epoch:  92 (100%)\tLoss:\t0.0090\trLoss: 791.64\tTrain Acc: 0.9820 Test Acc: 0.7992\tTime: 40.3988\n",
      "Train Epoch:  93 (100%)\tLoss:\t0.0050\trLoss: 827.70\tTrain Acc: 0.9808 Test Acc: 0.7542\tTime: 40.7045\n",
      "Train Epoch:  94 (100%)\tLoss:\t0.0080\trLoss: 874.90\tTrain Acc: 0.9804 Test Acc: 0.7708\tTime: 40.3702\n",
      "Train Epoch:  95 (100%)\tLoss:\t0.0585\trLoss: 872.06\tTrain Acc: 0.9799 Test Acc: 0.7661\tTime: 40.7160\n",
      "Train Epoch:  96 (100%)\tLoss:\t0.0029\trLoss: 888.30\tTrain Acc: 0.9791 Test Acc: 0.7729\tTime: 40.4229\n",
      "Train Epoch:  97 (100%)\tLoss:\t0.2581\trLoss: 911.69\tTrain Acc: 0.9781 Test Acc: 0.7665\tTime: 40.4613\n",
      "Train Epoch:  98 (100%)\tLoss:\t0.0052\trLoss: 794.15\tTrain Acc: 0.9815 Test Acc: 0.7517\tTime: 40.5323\n",
      "Train Epoch:  99 (100%)\tLoss:\t0.0527\trLoss: 834.24\tTrain Acc: 0.9813 Test Acc: 0.7898\tTime: 40.5862\n",
      "Train Epoch: 100 (100%)\tLoss:\t0.0084\trLoss: 892.69\tTrain Acc: 0.9794 Test Acc: 0.7788\tTime: 40.5464\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "\n",
    "model = DeepClassifier(k=cfgs['vgg16'], batchnorm=False, lt_dim=12).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=1E-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    start = time.time()\n",
    "    train_l, train_c, train_s = train(model, train_dataloader, optimizer, epoch, criterion)\n",
    "    test_l,  test_c           = test(model,  test_dataloader,  optimizer, epoch, criterion)\n",
    "    t = time.time() - start\n",
    "    print(train_s, 'Test Acc: {:.4f}\\tTime: {:.4f}'.format(test_c, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'vgg16_lt_12_norot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o = model(sample[0].to(device))\n",
    "# o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.sum(F.softmax(o[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 0.0001\n",
    "\n",
    "# model = DeepClassifier(k=cfgs['vgg16'], batchnorm=False, lt_dim=12, n_angles=4).to(device)\n",
    "# optimizer = Adam(model.parameters(), lr=lr, weight_decay=1E-5)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# for epoch in range(1, epochs+1):\n",
    "#     start = time.time()rLoss: 7505.07\tTrain Acc: 0.8369 Test Acc: 0.7767\tTime: 40.5210\n",
    "# Train Epoch:  20 (100%)\tLoss:\t0.1337\trLoss: 7193.00\tTrain Acc: 0.8425 Test Acc: 0.7547\tTime: 40.5516\n",
    "# Train Epoch:  21 (100%)\tLoss:\t0.1812\trLoss: 6788.13\tTrain Acc: 0.8538 Test Acc: 0.7678\tTime: 40.5076\n",
    "# Train Epoch:  22 (100%)\tLoss:\t0.2032\trLoss: 6645.48\tTrain Acc: 0.8610 Test Acc: 0.7648\tTime: 40.6877\n",
    "# Train Epoch:  23 (100%)\tLoss:\t0.2162\trLoss: 6335.58\tTrain Acc: 0.8676 Test Acc: 0.7733\tTime: 40.5185\n",
    "# Train Epoch:  24 (100%)\tLoss:\t0.6077\trLoss: 6069.68\tTrain Acc: 0.8724 Test Acc: 0.7818\tTime: 40.7110\n",
    "# Train Epoch:  25 (100%)\tLoss:\t0.6024\trLoss: 5887.14\tTrain Acc: 0.8759 Test Acc: 0.7487\tTime: 40.5745\n",
    "# Train Epoch:  26 (100%)\tLoss:\t0.3415\trLoss: 5956.25\tTrain Acc: 0.8758 Test Acc: 0.7445\tTime: 40.4299\n",
    "# Train Epoch:  27 (100%)\tLoss:\t0.3464\trLoss: 5626.56\tTrain Acc: 0.8816 Test Acc: 0.7843\tTime: 40.3893\n",
    "# Train Epoch:  28 (100%)\tLoss:\t0.2469\trLoss: 5430.75\tTrain Acc: 0.8885 Test Acc: 0.7678\tTime: 40.6997\n",
    "# Train Epoch:  29 (100%)\tLoss:\t0.6321\trLoss: 5045.21\tTrain Acc: 0.8989 Test Acc: 0.7797\tTime: 40.5346\n",
    "# Train Epoch:  30 (100%)\tLoss:\t0.3785\trLoss: 4800.74\tTrain Acc: 0.9014 Test Acc: 0.7568\tTime: 40.3140\n",
    "# Train Epoch:  31 (100%)\tLoss:\t0.3300\trLoss: 4469.81\tTrain Acc: 0.9067 Test Acc: 0.7699\tTime: 40.4569\n",
    "# Train Epoch:  32 (100%)\tLoss:\t0.4838\trLoss: 3952.21\tTrain Acc: 0.9200 Test Acc: 0.7653\tTime: 40.3864\n",
    "# Train Epoch:  33 (100%)\tLoss:\t0.1040\trLoss: 4139.05\tTrain Acc: 0.9180 Test Acc: 0.7903\tTime: 40.4503\n",
    "# Train Epoch:  34 (100%)\tLoss:\t0.0622\trLoss: 3771.18\tTrain Acc: 0.9259 Test Acc: 0.7508\tTime: 40.4396\n",
    "# Train Epoch:  35 (100%)\tLoss:\t0.2242\trLoss: 3817.37\tTrain Acc: 0.9225 Test Acc: 0.7297\tTime: 40.2514\n",
    "# Train Epoch:  36 (100%)\tLoss:\t0.1647\trLoss: 3382.78\tTrain Acc: 0.9344 Test Acc: 0.7826\tTime: 40.4183\n",
    "# Train Epoch:  37 (100%)\tLoss:\t0.2479\trLoss: 3396.16\tTrain Acc: 0.9304 Test Acc: 0.7691\tTime: 40.5690\n",
    "# Train Epoch:  38 (100%)\tLoss:\t0.1775\trLoss: 3277.08\tTrain Acc: 0.9319 Test Acc: 0.7436\tTime: 40.3225\n",
    "# Train Epoch:  39 (100%)\tLoss:\t0.1797\trLoss: 3146.23\tTrain Acc: 0.9360 Test Acc: 0.7568\tTime: 40.2505\n",
    "# Train Epoch:  40 (100%)\tLoss:\t0.1590\trLoss: 3058.00\tTrain Acc: 0.9363 Test Acc: 0.7725\tTime: 40.6033\n",
    "# Train Epoch:  41 (100%)\tLoss:\t0.2153\trLoss: 3023.08\tTrain Acc: 0.9370 Test Acc: 0.7864\tTime: 40.4619\n",
    "# Train Epoch:  42 (100%)\tLoss:\t0.2043\trLoss: 2723.91\tTrain Acc: 0.9451 Test Acc: 0.7369\tTime: 40.4325\n",
    "# Train Epoch:  43 (100%)\tLoss:\t0.0603\trLoss: 2770.46\tTrain Acc: 0.9418 Test Acc: 0.7869\tTime: 40.5331\n",
    "# Train Epoch:  44 (100%)\tLoss:\t0.1958\trLoss: 2530.57\tTrain Acc: 0.9489 Test Acc: 0.7780\tTime: 40.4032\n",
    "# Train Epoch:  45 (100%)\tLoss:\t0.0231\trLoss: 2440.84\tTrain Acc: 0.9474\n",
    "# In [ ]:\n",
    "# ￼\n",
    "# torch.save(model, 'vgg16_lt_12_norot')\n",
    "# In [ ]:\n",
    "# ￼\n",
    "# lr = 0.0001\n",
    "# ​\n",
    "# model = DeepClassifier(k=cfgs['vgg16'], batchnorm=False, lt_dim=12, n_angles=4).to(device)\n",
    "# optimizer = Adam(model.parameters(), lr=lr, weight_decay=1E-5)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# ​\n",
    "# for epoch in range(1, epochs+1):\n",
    "#     start = time.time()\n",
    "#     train_l, train_c, train_s = train(model, train_dataloader, optimizer, epoch, criterion)\n",
    "#     test_l,  test_c           = test(model,  test_dataloader,  optimizer, epoch, criterion)\n",
    "#     t = time.time() - start\n",
    "#     print(train_s, 'Test Acc: {:.4f}\\tTime: {:.4f}'.format(test_c, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch.save(model, 'vgg16_lt_12_4_rot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
