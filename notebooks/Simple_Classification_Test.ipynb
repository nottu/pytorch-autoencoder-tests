{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas import read_fwf, DataFrame\n",
    "from tqdm   import tqdm_notebook as tqdm\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "from skimage import exposure\n",
    "from skimage import measure\n",
    "from skimage.io import imsave\n",
    "from skimage.filters import gaussian as gaussian_filter\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage import filters\n",
    "from skimage.morphology import opening, closing, disk, binary_dilation, flood\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "from VAE.rg_dataset import LRG, BasicDataset, get_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "aug = 10\n",
    "data_path = '../data/'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-087d49335d6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#                      file_dir=data_path + 'lrg')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlrg_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m112\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrd_sz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_aug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# lrg_data_set   = LRG(112, rd_sz=128, use_kittler=True, n_aug=aug, blur=False, \n",
    "#                      catalog_dir=data_path + 'catalog/mrt-table3.txt', \n",
    "#                      file_dir=data_path + 'lrg')\n",
    "\n",
    "lrg_datasets = get_datasets(112, rd_sz=128, n_aug=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = lrg_data_set.get_data()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = BasicDataset(X_train, y_train, n_aug=15, sz=112) #\n",
    "# test_dataset  = BasicDataset(X_test,  y_test,  n_aug=5,  sz=112)\n",
    "\n",
    "train_dataloader = data.DataLoader(lrg_datasets['train'], batch_size=batch_size, shuffle=True)\n",
    "test_dataloader  = data.DataLoader(lrg_datasets['test'],  batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = iter(test_dataloader).next()\n",
    "plt.imshow(sample[0].numpy()[0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepClassifier(nn.Module):\n",
    "    ''' Somewhat inspired in VGG'''\n",
    "    def __init__(self, k=None, num_classes=6, lt_dim=8, batchnorm=True, in_channels=1, non_linearity=nn.ReLU,\n",
    "                 Conv2d=nn.Conv2d, MaxPool2d=nn.MaxPool2d, BatchNorm2d=nn.BatchNorm2d, n_angles=1):\n",
    "        super(DeepClassifier, self).__init__()\n",
    "        \n",
    "        self.non_linearity = non_linearity\n",
    "        self.Conv2d = Conv2d\n",
    "        self.MaxPool2d = MaxPool2d\n",
    "        self.BatchNorm2d = BatchNorm2d\n",
    "        self.n_angles = n_angles\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(k[-2] * 7 * 7, lt_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(lt_dim, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "        if k == None:\n",
    "            k = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "        layers = self.__make_layers(k, batchnorm)\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def __make_layers(self, k, batch_norm, in_channels=1):\n",
    "        layers = []\n",
    "        for v in k:\n",
    "            if v == 'M' :\n",
    "                layers += [self.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else :\n",
    "                conv2d = self.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, self.BatchNorm2d(v), self.non_linearity(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, self.non_linearity(inplace=True)]\n",
    "                in_channels = v\n",
    "#         for l in layers: print(l)\n",
    "        return layers\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def rotate_input(self, x, angle):\n",
    "        if angle == 0 or angle == 360 :\n",
    "            return x\n",
    "        bsz = x.shape[0]\n",
    "\n",
    "        r = torch.zeros([bsz, 2, 3], dtype=torch.float32, device='cuda')\n",
    "        r[:, 0, 0] =    torch.cos(torch.FloatTensor(bsz).fill_(angle))\n",
    "        r[:, 0, 1] =    torch.sin(torch.FloatTensor(bsz).fill_(angle))\n",
    "        r[:, 1, 0] = -1*torch.sin(torch.FloatTensor(bsz).fill_(angle))\n",
    "        r[:, 1, 1] =    torch.cos(torch.FloatTensor(bsz).fill_(angle))\n",
    "        \n",
    "        grid = F.affine_grid(r, x.size()).cuda()\n",
    "        x = F.grid_sample(x, grid, padding_mode='zeros')\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        ys = []\n",
    "        angle_steps = 360/self.n_angles\n",
    "        angle = 0\n",
    "        for a in range(self.n_angles):\n",
    "            x = self.rotate_input(x, angle)\n",
    "            y = self.features(x)\n",
    "            y = self.avgpool(y)\n",
    "            y = torch.flatten(y, 1)\n",
    "            y = self.classifier(y)\n",
    "#             y = F.softmax(y)\n",
    "            ys.append(y)\n",
    "            angle += angle_steps\n",
    "        y = torch.mean(torch.stack(ys), dim=0)\n",
    "        return y\n",
    "\n",
    "'''Common configs for VGG like networks'''\n",
    "cfgs = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, epoch, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    running_inputs=0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        running_inputs += len(labels.data)\n",
    "        \n",
    "        running_acc = (1.0 * running_corrects)/running_inputs\n",
    "        \n",
    "        s = 'Train Epoch: {:3d} ({:3.0f}%)\\tLoss:\\t{:4.4f}\\trLoss: {:4.2f}\\tTrain Acc: {:.4f}'\n",
    "        s = s.format(epoch,\n",
    "                100. * batch_idx / len(dataloader), loss.item(), running_loss, running_acc)\n",
    "        sys.stdout.write('{}\\r'.format(s))\n",
    "        sys.stdout.flush()\n",
    "    return running_loss, running_corrects, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, optimizer, epoch, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    running_inputs=0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        running_inputs += len(labels.data)\n",
    "        \n",
    "        running_acc = (1.0 * running_corrects)/running_inputs\n",
    "        \n",
    "#         s = 'Test Epoch: {:3d} ({:3.0f}%)\\tLoss:\\t{:4.4f}\\trLoss: {:4.2f}\\trPreds: {:.4f}'\n",
    "#         s = s.format(epoch,\n",
    "#                 100. * batch_idx / len(dataloader), loss.item(), running_loss, running_acc)\n",
    "#         sys.stdout.write('{}\\r'.format(s))\n",
    "#         sys.stdout.flush()\n",
    "\n",
    "    return running_loss, running_acc#, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:   1 (100%)\tLoss:\t0.1285\trLoss: 3567.75\tTrain Acc: 0.9023 Test Acc: 0.9405\tTime: 11.1774\n",
      "Train Epoch:   2 (100%)\tLoss:\t0.1193\trLoss: 2587.78\tTrain Acc: 0.9296 Test Acc: 0.9499\tTime: 11.0961\n",
      "Train Epoch:   3 (100%)\tLoss:\t0.1167\trLoss: 2301.29\tTrain Acc: 0.9357 Test Acc: 0.9454\tTime: 11.1540\n",
      "Train Epoch:   4 (100%)\tLoss:\t0.0522\trLoss: 2012.99\tTrain Acc: 0.9426 Test Acc: 0.9454\tTime: 10.8532\n",
      "Train Epoch:   5 (100%)\tLoss:\t0.1563\trLoss: 1839.45\tTrain Acc: 0.9481 Test Acc: 0.9352\tTime: 10.7340\n",
      "Train Epoch:   6 (100%)\tLoss:\t0.0728\trLoss: 1680.48\tTrain Acc: 0.9522 Test Acc: 0.9407\tTime: 10.8790\n",
      "Train Epoch:   7 (100%)\tLoss:\t0.0818\trLoss: 1601.57\tTrain Acc: 0.9562 Test Acc: 0.9548\tTime: 10.9237\n",
      "Train Epoch:   8 (100%)\tLoss:\t0.2467\trLoss: 1590.73\tTrain Acc: 0.9548 Test Acc: 0.9481\tTime: 10.9950\n",
      "Train Epoch:   9 (100%)\tLoss:\t0.2129\trLoss: 1398.78\tTrain Acc: 0.9614 Test Acc: 0.9547\tTime: 10.6804\n",
      "Train Epoch:  10 (100%)\tLoss:\t0.0951\trLoss: 1386.92\tTrain Acc: 0.9608 Test Acc: 0.9362\tTime: 11.0466\n",
      "Train Epoch:  11 (100%)\tLoss:\t0.0877\trLoss: 1277.29\tTrain Acc: 0.9646 Test Acc: 0.9474\tTime: 10.8090\n",
      "Train Epoch:  12 (100%)\tLoss:\t0.0718\trLoss: 1303.89\tTrain Acc: 0.9640 Test Acc: 0.8653\tTime: 10.8178\n",
      "Train Epoch:  13 (100%)\tLoss:\t0.0995\trLoss: 1294.65\tTrain Acc: 0.9645 Test Acc: 0.9531\tTime: 11.0486\n",
      "Train Epoch:  14 (100%)\tLoss:\t0.0139\trLoss: 1198.26\tTrain Acc: 0.9675 Test Acc: 0.9395\tTime: 10.9076\n",
      "Train Epoch:  15 (100%)\tLoss:\t0.0867\trLoss: 1184.51\tTrain Acc: 0.9679 Test Acc: 0.9534\tTime: 10.9153\n",
      "Train Epoch:  16 (100%)\tLoss:\t0.0162\trLoss: 1109.52\tTrain Acc: 0.9719 Test Acc: 0.9486\tTime: 10.9963\n",
      "Train Epoch:  17 (100%)\tLoss:\t0.0299\trLoss: 1096.74\tTrain Acc: 0.9722 Test Acc: 0.9451\tTime: 11.0619\n",
      "Train Epoch:  18 (100%)\tLoss:\t0.0836\trLoss: 1144.58\tTrain Acc: 0.9690 Test Acc: 0.9459\tTime: 10.9101\n",
      "Train Epoch:  19 (100%)\tLoss:\t0.1463\trLoss: 1073.78\tTrain Acc: 0.9724 Test Acc: 0.9470\tTime: 11.0209\n",
      "Train Epoch:  20 (100%)\tLoss:\t0.0461\trLoss: 1023.42\tTrain Acc: 0.9723 Test Acc: 0.9306\tTime: 11.0107\n",
      "Train Epoch:  21 (100%)\tLoss:\t0.0906\trLoss: 1042.29\tTrain Acc: 0.9738 Test Acc: 0.9277\tTime: 10.7397\n",
      "Train Epoch:  22 (100%)\tLoss:\t0.0878\trLoss: 974.16\tTrain Acc: 0.9760 Test Acc: 0.9467\tTime: 10.7415\n",
      "Train Epoch:  23 (100%)\tLoss:\t0.0141\trLoss: 1031.51\tTrain Acc: 0.9729 Test Acc: 0.9437\tTime: 11.0893\n",
      "Train Epoch:  24 (100%)\tLoss:\t0.0380\trLoss: 1029.64\tTrain Acc: 0.9732 Test Acc: 0.9502\tTime: 11.0293\n",
      "Train Epoch:  25 (100%)\tLoss:\t0.0154\trLoss: 968.14\tTrain Acc: 0.9753 Test Acc: 0.9350\tTime: 10.4870\n",
      "Train Epoch:  26 (100%)\tLoss:\t0.0090\trLoss: 982.96\tTrain Acc: 0.9744 Test Acc: 0.9558\tTime: 10.7827\n",
      "Train Epoch:  27 (100%)\tLoss:\t0.1714\trLoss: 972.23\tTrain Acc: 0.9760 Test Acc: 0.9481\tTime: 10.7288\n",
      "Train Epoch:  28 (100%)\tLoss:\t0.2479\trLoss: 916.56\tTrain Acc: 0.9776 Test Acc: 0.9505\tTime: 10.6671\n",
      "Train Epoch:  29 (100%)\tLoss:\t0.0062\trLoss: 896.45\tTrain Acc: 0.9783 Test Acc: 0.9470\tTime: 10.7880\n",
      "Train Epoch:  30 (100%)\tLoss:\t0.0999\trLoss: 880.37\tTrain Acc: 0.9783 Test Acc: 0.9526\tTime: 10.9061\n",
      "Train Epoch:  31 (100%)\tLoss:\t0.0340\trLoss: 773.72\tTrain Acc: 0.9821 Test Acc: 0.9510\tTime: 11.2293\n",
      "Train Epoch:  32 (100%)\tLoss:\t0.1001\trLoss: 735.08\tTrain Acc: 0.9832 Test Acc: 0.9512\tTime: 10.8077\n",
      "Train Epoch:  33 (100%)\tLoss:\t0.0192\trLoss: 766.47\tTrain Acc: 0.9827 Test Acc: 0.9521\tTime: 11.0062\n",
      "Train Epoch:  34 (100%)\tLoss:\t0.1190\trLoss: 656.30\tTrain Acc: 0.9851 Test Acc: 0.9537\tTime: 11.1292\n",
      "Train Epoch:  35 (100%)\tLoss:\t0.0349\trLoss: 674.55\tTrain Acc: 0.9847 Test Acc: 0.9551\tTime: 10.9273\n",
      "Train Epoch:  36 (100%)\tLoss:\t0.0056\trLoss: 655.18\tTrain Acc: 0.9858 Test Acc: 0.9537\tTime: 10.7801\n",
      "Train Epoch:  37 (100%)\tLoss:\t0.0874\trLoss: 674.67\tTrain Acc: 0.9847 Test Acc: 0.9532\tTime: 10.9451\n",
      "Train Epoch:  38 (100%)\tLoss:\t0.0098\trLoss: 641.32\tTrain Acc: 0.9857 Test Acc: 0.9516\tTime: 10.9454\n",
      "Train Epoch:  39 (100%)\tLoss:\t0.0083\trLoss: 636.08\tTrain Acc: 0.9858 Test Acc: 0.9522\tTime: 10.6379\n",
      "Train Epoch:  40 (100%)\tLoss:\t0.0015\trLoss: 639.27\tTrain Acc: 0.9853 Test Acc: 0.9523\tTime: 10.8774\n",
      "Train Epoch:  41 (100%)\tLoss:\t0.0105\trLoss: 634.68\tTrain Acc: 0.9864 Test Acc: 0.9525\tTime: 10.8720\n",
      "Train Epoch:  42 (100%)\tLoss:\t0.0178\trLoss: 595.04\tTrain Acc: 0.9874 Test Acc: 0.9538\tTime: 10.6428\n",
      "Train Epoch:  43 (100%)\tLoss:\t0.0111\trLoss: 614.29\tTrain Acc: 0.9875 Test Acc: 0.9569\tTime: 10.7655\n",
      "Train Epoch:  44 (100%)\tLoss:\t0.0013\trLoss: 606.11\tTrain Acc: 0.9874 Test Acc: 0.9556\tTime: 10.6526\n",
      "Train Epoch:  45 (100%)\tLoss:\t0.0089\trLoss: 608.30\tTrain Acc: 0.9871 Test Acc: 0.9547\tTime: 10.6749\n",
      "Train Epoch:  46 (100%)\tLoss:\t0.1043\trLoss: 607.75\tTrain Acc: 0.9870 Test Acc: 0.9551\tTime: 10.6567\n",
      "Train Epoch:  47 (100%)\tLoss:\t0.0041\trLoss: 638.65\tTrain Acc: 0.9859 Test Acc: 0.9551\tTime: 10.7852\n",
      "Train Epoch:  48 ( 75%)\tLoss:\t0.0012\trLoss: 457.89\tTrain Acc: 0.9867\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-5d11438afe15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtest_l\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtest_c\u001b[0m           \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-59d5d0ad70ad>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, epoch, criterion)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = [16, 'M', 32, 'M', 64, 64, 'M', 128, 128, 'M']\n",
    "model = DeepClassifier(k=k, batchnorm=True, lt_dim=12, num_classes=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=1E-5)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    start = time.time()\n",
    "    train_l, train_c, train_s = train(model, train_dataloader, optimizer, epoch, criterion)\n",
    "    test_l,  test_c           = test(model,  test_dataloader,  optimizer, epoch, criterion)\n",
    "    t = time.time() - start\n",
    "    print(train_s, 'Test Acc: {:.4f}\\tTime: {:.4f}'.format(test_c, t))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model(sample[0].to(device))\n",
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'vgg16_lt_12_norot_slim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:   1 (100%)\tLoss:\t1.1146\trLoss: 18420.56\tTrain Acc: 0.4613 Test Acc: 0.6415\tTime: 40.1315\n",
      "Train Epoch:   2 (100%)\tLoss:\t0.9460\trLoss: 14230.20\tTrain Acc: 0.6266 Test Acc: 0.5695\tTime: 40.3830\n",
      "Train Epoch:   3 (100%)\tLoss:\t0.7905\trLoss: 13208.31\tTrain Acc: 0.6406 Test Acc: 0.6347\tTime: 40.5623\n",
      "Train Epoch:   4 (100%)\tLoss:\t0.7198\trLoss: 12184.03\tTrain Acc: 0.6690 Test Acc: 0.7161\tTime: 40.3843\n",
      "Train Epoch:   5 (100%)\tLoss:\t1.2461\trLoss: 11606.61\tTrain Acc: 0.6936 Test Acc: 0.7144\tTime: 40.5554\n",
      "Train Epoch:   6 (100%)\tLoss:\t1.1252\trLoss: 10938.09\tTrain Acc: 0.7178 Test Acc: 0.7305\tTime: 40.6335\n",
      "Train Epoch:   7 (100%)\tLoss:\t0.7188\trLoss: 10146.23\tTrain Acc: 0.7433 Test Acc: 0.7458\tTime: 40.5927\n",
      "Train Epoch:   8 (100%)\tLoss:\t0.9503\trLoss: 9725.38\tTrain Acc: 0.7551 Test Acc: 0.7326\tTime: 40.6760\n",
      "Train Epoch:   9 (100%)\tLoss:\t0.2827\trLoss: 8983.03\tTrain Acc: 0.7760 Test Acc: 0.7415\tTime: 40.8584\n",
      "Train Epoch:  10 (100%)\tLoss:\t0.7002\trLoss: 8405.91\tTrain Acc: 0.7900 Test Acc: 0.7606\tTime: 40.7824\n",
      "Train Epoch:  11 (100%)\tLoss:\t0.5626\trLoss: 7793.21\tTrain Acc: 0.8058 Test Acc: 0.7949\tTime: 40.5495\n",
      "Train Epoch:  12 (100%)\tLoss:\t0.1587\trLoss: 7520.59\tTrain Acc: 0.8148 Test Acc: 0.7907\tTime: 40.6665\n",
      "Train Epoch:  13 (100%)\tLoss:\t0.2972\trLoss: 6979.88\tTrain Acc: 0.8291 Test Acc: 0.7928\tTime: 40.7018\n",
      "Train Epoch:  14 (100%)\tLoss:\t0.2414\trLoss: 6431.86\tTrain Acc: 0.8431 Test Acc: 0.7869\tTime: 40.7318\n",
      "Train Epoch:  15 (100%)\tLoss:\t0.3743\trLoss: 6066.06\tTrain Acc: 0.8518 Test Acc: 0.7809\tTime: 40.8320\n",
      "Train Epoch:  16 (100%)\tLoss:\t0.4717\trLoss: 5649.03\tTrain Acc: 0.8637 Test Acc: 0.7958\tTime: 40.8124\n",
      "Train Epoch:  17 (100%)\tLoss:\t0.4986\trLoss: 5353.52\tTrain Acc: 0.8707 Test Acc: 0.7818\tTime: 40.8760\n",
      "Train Epoch:  18 (100%)\tLoss:\t0.2071\trLoss: 5205.65\tTrain Acc: 0.8727 Test Acc: 0.7847\tTime: 41.0257\n",
      "Train Epoch:  19 (100%)\tLoss:\t0.2373\trLoss: 4700.57\tTrain Acc: 0.8862 Test Acc: 0.7873\tTime: 40.7765\n",
      "Train Epoch:  20 (100%)\tLoss:\t0.1965\trLoss: 4394.77\tTrain Acc: 0.8931 Test Acc: 0.7941\tTime: 40.8792\n",
      "Train Epoch:  21 (100%)\tLoss:\t0.4001\trLoss: 4233.75\tTrain Acc: 0.8954 Test Acc: 0.7733\tTime: 40.9003\n",
      "Train Epoch:  22 (100%)\tLoss:\t0.1101\trLoss: 3879.83\tTrain Acc: 0.9041 Test Acc: 0.7619\tTime: 40.8044\n",
      "Train Epoch:  23 (100%)\tLoss:\t0.5359\trLoss: 3637.61\tTrain Acc: 0.9099 Test Acc: 0.7669\tTime: 40.9511\n",
      "Train Epoch:  24 (100%)\tLoss:\t0.0804\trLoss: 3388.37\tTrain Acc: 0.9165 Test Acc: 0.7907\tTime: 40.9764\n",
      "Train Epoch:  25 (100%)\tLoss:\t0.2321\trLoss: 3348.37\tTrain Acc: 0.9181 Test Acc: 0.7725\tTime: 40.9592\n",
      "Train Epoch:  26 (100%)\tLoss:\t0.5031\trLoss: 3154.69\tTrain Acc: 0.9243 Test Acc: 0.7780\tTime: 41.0565\n",
      "Train Epoch:  27 (100%)\tLoss:\t0.1901\trLoss: 2896.24\tTrain Acc: 0.9296 Test Acc: 0.7538\tTime: 41.0304\n",
      "Train Epoch:  28 (100%)\tLoss:\t0.0541\trLoss: 2979.69\tTrain Acc: 0.9266 Test Acc: 0.7733\tTime: 40.8497\n",
      "Train Epoch:  29 (100%)\tLoss:\t0.1852\trLoss: 2641.57\tTrain Acc: 0.9333 Test Acc: 0.7301\tTime: 40.9014\n",
      "Train Epoch:  30 (100%)\tLoss:\t0.2437\trLoss: 2733.92\tTrain Acc: 0.9339 Test Acc: 0.7602\tTime: 40.7609\n",
      "Train Epoch:  31 (100%)\tLoss:\t0.1358\trLoss: 2517.92\tTrain Acc: 0.9383 Test Acc: 0.8110\tTime: 41.1560\n",
      "Train Epoch:  32 (100%)\tLoss:\t0.1207\trLoss: 2492.63\tTrain Acc: 0.9383 Test Acc: 0.7864\tTime: 41.0823\n",
      "Train Epoch:  33 (100%)\tLoss:\t0.0518\trLoss: 2402.88\tTrain Acc: 0.9418 Test Acc: 0.7852\tTime: 40.6368\n",
      "Train Epoch:  34 (100%)\tLoss:\t0.2562\trLoss: 2463.32\tTrain Acc: 0.9411 Test Acc: 0.7801\tTime: 40.8035\n",
      "Train Epoch:  35 (100%)\tLoss:\t0.1914\trLoss: 2444.92\tTrain Acc: 0.9397 Test Acc: 0.7648\tTime: 40.7769\n",
      "Train Epoch:  36 (100%)\tLoss:\t0.0919\trLoss: 2078.62\tTrain Acc: 0.9489 Test Acc: 0.7606\tTime: 40.8489\n",
      "Train Epoch:  37 (100%)\tLoss:\t0.0429\trLoss: 2478.80\tTrain Acc: 0.9420 Test Acc: 0.7513\tTime: 40.9274\n",
      "Train Epoch:  38 (100%)\tLoss:\t0.3975\trLoss: 2011.03\tTrain Acc: 0.9527 Test Acc: 0.7021\tTime: 40.8816\n",
      "Train Epoch:  39 (100%)\tLoss:\t0.0142\trLoss: 2010.66\tTrain Acc: 0.9544 Test Acc: 0.7551\tTime: 40.7777\n",
      "Train Epoch:  40 (100%)\tLoss:\t0.0165\trLoss: 1858.81\tTrain Acc: 0.9551 Test Acc: 0.7538\tTime: 40.8242\n",
      "Train Epoch:  41 (100%)\tLoss:\t0.1673\trLoss: 1820.96\tTrain Acc: 0.9597 Test Acc: 0.7801\tTime: 40.8002\n",
      "Train Epoch:  42 (100%)\tLoss:\t0.0467\trLoss: 1979.15\tTrain Acc: 0.9546 Test Acc: 0.7614\tTime: 40.8698\n",
      "Train Epoch:  43 (100%)\tLoss:\t0.1751\trLoss: 1911.77\tTrain Acc: 0.9580 Test Acc: 0.7640\tTime: 40.9452\n",
      "Train Epoch:  44 (100%)\tLoss:\t0.0071\trLoss: 1816.22\tTrain Acc: 0.9591 Test Acc: 0.7280\tTime: 40.8096\n",
      "Train Epoch:  45 (100%)\tLoss:\t0.0858\trLoss: 1767.95\tTrain Acc: 0.9615 Test Acc: 0.7534\tTime: 40.6675\n",
      "Train Epoch:  46 (100%)\tLoss:\t0.2208\trLoss: 1825.12\tTrain Acc: 0.9588 Test Acc: 0.7716\tTime: 41.0032\n",
      "Train Epoch:  47 (100%)\tLoss:\t0.0322\trLoss: 1726.47\tTrain Acc: 0.9609 Test Acc: 0.7538\tTime: 40.9723\n",
      "Train Epoch:  48 (100%)\tLoss:\t0.0324\trLoss: 1559.10\tTrain Acc: 0.9654 Test Acc: 0.7415\tTime: 40.8669\n",
      "Train Epoch:  49 (100%)\tLoss:\t0.0501\trLoss: 1720.74\tTrain Acc: 0.9611 Test Acc: 0.7695\tTime: 40.9708\n",
      "Train Epoch:  50 (100%)\tLoss:\t0.0125\trLoss: 1510.87\tTrain Acc: 0.9656 Test Acc: 0.7496\tTime: 40.7518\n",
      "Train Epoch:  51 (100%)\tLoss:\t0.0373\trLoss: 1340.15\tTrain Acc: 0.9688 Test Acc: 0.7775\tTime: 40.7595\n",
      "Train Epoch:  52 (100%)\tLoss:\t0.0045\trLoss: 1626.07\tTrain Acc: 0.9636 Test Acc: 0.7568\tTime: 40.6327\n",
      "Train Epoch:  53 (100%)\tLoss:\t0.0084\trLoss: 1565.20\tTrain Acc: 0.9651 Test Acc: 0.7818\tTime: 40.7251\n",
      "Train Epoch:  54 (100%)\tLoss:\t0.0472\trLoss: 1677.35\tTrain Acc: 0.9624 Test Acc: 0.7737\tTime: 40.7880\n",
      "Train Epoch:  55 (100%)\tLoss:\t0.0173\trLoss: 1407.54\tTrain Acc: 0.9696 Test Acc: 0.7699\tTime: 40.9103\n",
      "Train Epoch:  56 (100%)\tLoss:\t0.2017\trLoss: 1400.69\tTrain Acc: 0.9689 Test Acc: 0.7441\tTime: 40.8448\n",
      "Train Epoch:  57 (100%)\tLoss:\t0.1807\trLoss: 1464.81\tTrain Acc: 0.9678 Test Acc: 0.7496\tTime: 40.7996\n",
      "Train Epoch:  58 (100%)\tLoss:\t0.1368\trLoss: 1311.60\tTrain Acc: 0.9696 Test Acc: 0.7441\tTime: 40.8414\n",
      "Train Epoch:  59 (100%)\tLoss:\t0.0050\trLoss: 1521.56\tTrain Acc: 0.9656 Test Acc: 0.7386\tTime: 40.7731\n",
      "Train Epoch:  60 (100%)\tLoss:\t0.0481\trLoss: 1466.74\tTrain Acc: 0.9650 Test Acc: 0.7699\tTime: 40.8075\n",
      "Train Epoch:  61 (100%)\tLoss:\t0.0103\trLoss: 1400.73\tTrain Acc: 0.9678 Test Acc: 0.7398\tTime: 40.8708\n",
      "Train Epoch:  62 (100%)\tLoss:\t0.0828\trLoss: 1331.82\tTrain Acc: 0.9693 Test Acc: 0.7339\tTime: 40.7391\n",
      "Train Epoch:  63 (100%)\tLoss:\t0.0470\trLoss: 1139.97\tTrain Acc: 0.9753 Test Acc: 0.7716\tTime: 40.6681\n",
      "Train Epoch:  64 (100%)\tLoss:\t0.0687\trLoss: 1554.19\tTrain Acc: 0.9647 Test Acc: 0.7318\tTime: 40.6499\n",
      "Train Epoch:  65 (100%)\tLoss:\t0.0104\trLoss: 1321.50\tTrain Acc: 0.9693 Test Acc: 0.7538\tTime: 40.5930\n",
      "Train Epoch:  66 (100%)\tLoss:\t0.0140\trLoss: 1353.89\tTrain Acc: 0.9696 Test Acc: 0.7627\tTime: 40.6499\n",
      "Train Epoch:  67 (100%)\tLoss:\t0.2367\trLoss: 1303.66\tTrain Acc: 0.9702 Test Acc: 0.7165\tTime: 40.7140\n",
      "Train Epoch:  68 (100%)\tLoss:\t0.1859\trLoss: 1247.49\tTrain Acc: 0.9723 Test Acc: 0.7394\tTime: 40.7775\n",
      "Train Epoch:  69 (100%)\tLoss:\t0.0036\trLoss: 1136.12\tTrain Acc: 0.9739 Test Acc: 0.7581\tTime: 40.5891\n",
      "Train Epoch:  70 (100%)\tLoss:\t0.0010\trLoss: 1303.19\tTrain Acc: 0.9689 Test Acc: 0.7593\tTime: 40.6382\n",
      "Train Epoch:  71 (100%)\tLoss:\t0.1301\trLoss: 1271.47\tTrain Acc: 0.9705 Test Acc: 0.7653\tTime: 40.7559\n",
      "Train Epoch:  72 (100%)\tLoss:\t0.0345\trLoss: 1103.68\tTrain Acc: 0.9760 Test Acc: 0.7771\tTime: 40.7883\n",
      "Train Epoch:  73 (100%)\tLoss:\t0.0046\trLoss: 1126.33\tTrain Acc: 0.9746 Test Acc: 0.7373\tTime: 40.4700\n",
      "Train Epoch:  74 (100%)\tLoss:\t0.0166\trLoss: 1159.59\tTrain Acc: 0.9734 Test Acc: 0.7877\tTime: 40.6936\n",
      "Train Epoch:  75 (100%)\tLoss:\t0.0939\trLoss: 1141.52\tTrain Acc: 0.9738 Test Acc: 0.7758\tTime: 40.6877\n",
      "Train Epoch:  76 (100%)\tLoss:\t0.0400\trLoss: 1310.62\tTrain Acc: 0.9714 Test Acc: 0.7631\tTime: 40.7751\n",
      "Train Epoch:  77 (100%)\tLoss:\t0.0589\trLoss: 1325.57\tTrain Acc: 0.9683 Test Acc: 0.7831\tTime: 40.8507\n",
      "Train Epoch:  78 (100%)\tLoss:\t0.1950\trLoss: 1039.08\tTrain Acc: 0.9758 Test Acc: 0.7716\tTime: 40.7102\n",
      "Train Epoch:  79 (100%)\tLoss:\t0.0053\trLoss: 968.73\tTrain Acc: 0.9777 Test Acc: 0.7606\tTime: 40.6366\n",
      "Train Epoch:  80 (100%)\tLoss:\t0.1038\trLoss: 1139.22\tTrain Acc: 0.9750 Test Acc: 0.7564\tTime: 40.8014\n",
      "Train Epoch:  81 (100%)\tLoss:\t0.1483\trLoss: 1527.55\tTrain Acc: 0.9686 Test Acc: 0.7703\tTime: 40.8427\n",
      "Train Epoch:  82 (100%)\tLoss:\t0.0969\trLoss: 1024.44\tTrain Acc: 0.9767 Test Acc: 0.7432\tTime: 40.2589\n",
      "Train Epoch:  83 (100%)\tLoss:\t0.0008\trLoss: 1020.87\tTrain Acc: 0.9753 Test Acc: 0.7441\tTime: 40.3454\n",
      "Train Epoch:  84 (100%)\tLoss:\t0.1210\trLoss: 1103.44\tTrain Acc: 0.9742 Test Acc: 0.7699\tTime: 40.3215\n",
      "Train Epoch:  85 (100%)\tLoss:\t0.1051\trLoss: 1011.99\tTrain Acc: 0.9767 Test Acc: 0.7496\tTime: 40.2693\n",
      "Train Epoch:  86 (100%)\tLoss:\t0.0614\trLoss: 852.55\tTrain Acc: 0.9797 Test Acc: 0.7614\tTime: 40.3248\n",
      "Train Epoch:  87 (100%)\tLoss:\t0.0934\trLoss: 994.30\tTrain Acc: 0.9751 Test Acc: 0.7814\tTime: 40.7054\n",
      "Train Epoch:  88 (100%)\tLoss:\t0.0005\trLoss: 845.15\tTrain Acc: 0.9792 Test Acc: 0.7275\tTime: 40.6027\n",
      "Train Epoch:  89 (100%)\tLoss:\t0.1502\trLoss: 968.35\tTrain Acc: 0.9781 Test Acc: 0.7898\tTime: 40.5615\n",
      "Train Epoch:  90 (100%)\tLoss:\t0.3703\trLoss: 1061.91\tTrain Acc: 0.9764 Test Acc: 0.7843\tTime: 40.2928\n",
      "Train Epoch:  91 (100%)\tLoss:\t0.0512\trLoss: 998.88\tTrain Acc: 0.9775 Test Acc: 0.7742\tTime: 40.2446\n",
      "Train Epoch:  92 (100%)\tLoss:\t0.0090\trLoss: 791.64\tTrain Acc: 0.9820 Test Acc: 0.7992\tTime: 40.3988\n",
      "Train Epoch:  93 (100%)\tLoss:\t0.0050\trLoss: 827.70\tTrain Acc: 0.9808 Test Acc: 0.7542\tTime: 40.7045\n",
      "Train Epoch:  94 (100%)\tLoss:\t0.0080\trLoss: 874.90\tTrain Acc: 0.9804 Test Acc: 0.7708\tTime: 40.3702\n",
      "Train Epoch:  95 (100%)\tLoss:\t0.0585\trLoss: 872.06\tTrain Acc: 0.9799 Test Acc: 0.7661\tTime: 40.7160\n",
      "Train Epoch:  96 (100%)\tLoss:\t0.0029\trLoss: 888.30\tTrain Acc: 0.9791 Test Acc: 0.7729\tTime: 40.4229\n",
      "Train Epoch:  97 (100%)\tLoss:\t0.2581\trLoss: 911.69\tTrain Acc: 0.9781 Test Acc: 0.7665\tTime: 40.4613\n",
      "Train Epoch:  98 (100%)\tLoss:\t0.0052\trLoss: 794.15\tTrain Acc: 0.9815 Test Acc: 0.7517\tTime: 40.5323\n",
      "Train Epoch:  99 (100%)\tLoss:\t0.0527\trLoss: 834.24\tTrain Acc: 0.9813 Test Acc: 0.7898\tTime: 40.5862\n",
      "Train Epoch: 100 (100%)\tLoss:\t0.0084\trLoss: 892.69\tTrain Acc: 0.9794 Test Acc: 0.7788\tTime: 40.5464\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "\n",
    "model = DeepClassifier(k=cfgs['vgg16'], batchnorm=False, lt_dim=12).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=1E-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    start = time.time()\n",
    "    train_l, train_c, train_s = train(model, train_dataloader, optimizer, epoch, criterion)\n",
    "    test_l,  test_c           = test(model,  test_dataloader,  optimizer, epoch, criterion)\n",
    "    t = time.time() - start\n",
    "    print(train_s, 'Test Acc: {:.4f}\\tTime: {:.4f}'.format(test_c, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'vgg16_lt_12_norot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o = model(sample[0].to(device))\n",
    "# o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.sum(F.softmax(o[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 0.0001\n",
    "\n",
    "# model = DeepClassifier(k=cfgs['vgg16'], batchnorm=False, lt_dim=12, n_angles=4).to(device)\n",
    "# optimizer = Adam(model.parameters(), lr=lr, weight_decay=1E-5)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# for epoch in range(1, epochs+1):\n",
    "#     start = time.time()rLoss: 7505.07\tTrain Acc: 0.8369 Test Acc: 0.7767\tTime: 40.5210\n",
    "# Train Epoch:  20 (100%)\tLoss:\t0.1337\trLoss: 7193.00\tTrain Acc: 0.8425 Test Acc: 0.7547\tTime: 40.5516\n",
    "# Train Epoch:  21 (100%)\tLoss:\t0.1812\trLoss: 6788.13\tTrain Acc: 0.8538 Test Acc: 0.7678\tTime: 40.5076\n",
    "# Train Epoch:  22 (100%)\tLoss:\t0.2032\trLoss: 6645.48\tTrain Acc: 0.8610 Test Acc: 0.7648\tTime: 40.6877\n",
    "# Train Epoch:  23 (100%)\tLoss:\t0.2162\trLoss: 6335.58\tTrain Acc: 0.8676 Test Acc: 0.7733\tTime: 40.5185\n",
    "# Train Epoch:  24 (100%)\tLoss:\t0.6077\trLoss: 6069.68\tTrain Acc: 0.8724 Test Acc: 0.7818\tTime: 40.7110\n",
    "# Train Epoch:  25 (100%)\tLoss:\t0.6024\trLoss: 5887.14\tTrain Acc: 0.8759 Test Acc: 0.7487\tTime: 40.5745\n",
    "# Train Epoch:  26 (100%)\tLoss:\t0.3415\trLoss: 5956.25\tTrain Acc: 0.8758 Test Acc: 0.7445\tTime: 40.4299\n",
    "# Train Epoch:  27 (100%)\tLoss:\t0.3464\trLoss: 5626.56\tTrain Acc: 0.8816 Test Acc: 0.7843\tTime: 40.3893\n",
    "# Train Epoch:  28 (100%)\tLoss:\t0.2469\trLoss: 5430.75\tTrain Acc: 0.8885 Test Acc: 0.7678\tTime: 40.6997\n",
    "# Train Epoch:  29 (100%)\tLoss:\t0.6321\trLoss: 5045.21\tTrain Acc: 0.8989 Test Acc: 0.7797\tTime: 40.5346\n",
    "# Train Epoch:  30 (100%)\tLoss:\t0.3785\trLoss: 4800.74\tTrain Acc: 0.9014 Test Acc: 0.7568\tTime: 40.3140\n",
    "# Train Epoch:  31 (100%)\tLoss:\t0.3300\trLoss: 4469.81\tTrain Acc: 0.9067 Test Acc: 0.7699\tTime: 40.4569\n",
    "# Train Epoch:  32 (100%)\tLoss:\t0.4838\trLoss: 3952.21\tTrain Acc: 0.9200 Test Acc: 0.7653\tTime: 40.3864\n",
    "# Train Epoch:  33 (100%)\tLoss:\t0.1040\trLoss: 4139.05\tTrain Acc: 0.9180 Test Acc: 0.7903\tTime: 40.4503\n",
    "# Train Epoch:  34 (100%)\tLoss:\t0.0622\trLoss: 3771.18\tTrain Acc: 0.9259 Test Acc: 0.7508\tTime: 40.4396\n",
    "# Train Epoch:  35 (100%)\tLoss:\t0.2242\trLoss: 3817.37\tTrain Acc: 0.9225 Test Acc: 0.7297\tTime: 40.2514\n",
    "# Train Epoch:  36 (100%)\tLoss:\t0.1647\trLoss: 3382.78\tTrain Acc: 0.9344 Test Acc: 0.7826\tTime: 40.4183\n",
    "# Train Epoch:  37 (100%)\tLoss:\t0.2479\trLoss: 3396.16\tTrain Acc: 0.9304 Test Acc: 0.7691\tTime: 40.5690\n",
    "# Train Epoch:  38 (100%)\tLoss:\t0.1775\trLoss: 3277.08\tTrain Acc: 0.9319 Test Acc: 0.7436\tTime: 40.3225\n",
    "# Train Epoch:  39 (100%)\tLoss:\t0.1797\trLoss: 3146.23\tTrain Acc: 0.9360 Test Acc: 0.7568\tTime: 40.2505\n",
    "# Train Epoch:  40 (100%)\tLoss:\t0.1590\trLoss: 3058.00\tTrain Acc: 0.9363 Test Acc: 0.7725\tTime: 40.6033\n",
    "# Train Epoch:  41 (100%)\tLoss:\t0.2153\trLoss: 3023.08\tTrain Acc: 0.9370 Test Acc: 0.7864\tTime: 40.4619\n",
    "# Train Epoch:  42 (100%)\tLoss:\t0.2043\trLoss: 2723.91\tTrain Acc: 0.9451 Test Acc: 0.7369\tTime: 40.4325\n",
    "# Train Epoch:  43 (100%)\tLoss:\t0.0603\trLoss: 2770.46\tTrain Acc: 0.9418 Test Acc: 0.7869\tTime: 40.5331\n",
    "# Train Epoch:  44 (100%)\tLoss:\t0.1958\trLoss: 2530.57\tTrain Acc: 0.9489 Test Acc: 0.7780\tTime: 40.4032\n",
    "# Train Epoch:  45 (100%)\tLoss:\t0.0231\trLoss: 2440.84\tTrain Acc: 0.9474\n",
    "# In [ ]:\n",
    "# ￼\n",
    "# torch.save(model, 'vgg16_lt_12_norot')\n",
    "# In [ ]:\n",
    "# ￼\n",
    "# lr = 0.0001\n",
    "# ​\n",
    "# model = DeepClassifier(k=cfgs['vgg16'], batchnorm=False, lt_dim=12, n_angles=4).to(device)\n",
    "# optimizer = Adam(model.parameters(), lr=lr, weight_decay=1E-5)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# ​\n",
    "# for epoch in range(1, epochs+1):\n",
    "#     start = time.time()\n",
    "#     train_l, train_c, train_s = train(model, train_dataloader, optimizer, epoch, criterion)\n",
    "#     test_l,  test_c           = test(model,  test_dataloader,  optimizer, epoch, criterion)\n",
    "#     t = time.time() - start\n",
    "#     print(train_s, 'Test Acc: {:.4f}\\tTime: {:.4f}'.format(test_c, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch.save(model, 'vgg16_lt_12_4_rot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
