{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas import read_fwf, DataFrame\n",
    "from tqdm   import tqdm_notebook as tqdm\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "from skimage import exposure\n",
    "from skimage import measure\n",
    "from skimage.io import imsave\n",
    "from skimage.filters import gaussian as gaussian_filter\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage import filters\n",
    "from skimage.morphology import opening, closing, disk, binary_dilation, flood\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "from VAE.rg_dataset import LRG, BasicDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "aug = 10\n",
    "data_path = '../data/'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/lrg:\t1442/1442\n"
     ]
    }
   ],
   "source": [
    "lrg_data_set   = LRG(112, rd_sz=128, use_kittler=True, n_aug=aug, blur=False, \n",
    "                     catalog_dir=data_path + 'catalog/mrt-table3.txt', \n",
    "                     file_dir=data_path + 'lrg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "\n",
    "x = np.load(data_path+'proc_aligned2/lrg_norm_proc.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y = lrg_data_set.get_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BasicDataset(X_train, y_train, n_aug=5, sz=112, rotation=18) #\n",
    "test_dataset  = BasicDataset(X_test,  y_test,  n_aug=5, sz=112, rotation=18)\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader  = data.DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1429, 64, 64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = iter(test_dataloader).next()\n",
    "plt.imshow(sample[0].numpy()[10][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepClassifier(nn.Module):\n",
    "    ''' Somewhat inspired in VGG'''\n",
    "    def __init__(self, k=None, num_classes=6, lt_dim=8, batchnorm=True, in_channels=1, non_linearity=nn.ReLU,\n",
    "                 Conv2d=nn.Conv2d, MaxPool2d=nn.MaxPool2d, BatchNorm2d=nn.BatchNorm2d):\n",
    "        super(DeepClassifier, self).__init__()\n",
    "        \n",
    "        self.non_linearity = non_linearity\n",
    "        self.Conv2d = Conv2d\n",
    "        self.MaxPool2d = MaxPool2d\n",
    "        self.BatchNorm2d = BatchNorm2d\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(k[-2] * 7 * 7, lt_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(lt_dim, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "        if k == None:\n",
    "            k = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "        layers = self.__make_layers(k, batchnorm)\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def __make_layers(self, k, batch_norm, in_channels=1):\n",
    "        layers = []\n",
    "        for v in k:\n",
    "            if v == 'M' :\n",
    "                layers += [self.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else :\n",
    "                conv2d = self.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, self.BatchNorm2d(v), self.non_linearity(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, self.non_linearity(inplace=True)]\n",
    "                in_channels = v\n",
    "#         for l in layers: print(l)\n",
    "        return layers\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.features(x)\n",
    "        y = self.avgpool(y)\n",
    "        y = torch.flatten(y, 1)\n",
    "        y = self.classifier(y)\n",
    "        return y\n",
    "\n",
    "'''Common configs for VGG like networks'''\n",
    "cfgs = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, epoch, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    running_inputs=0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        running_inputs += len(labels.data)\n",
    "        \n",
    "        running_acc = (1.0 * running_corrects)/running_inputs\n",
    "        \n",
    "        s = 'Train Epoch: {:3d} ({:3.0f}%)\\tLoss:\\t{:4.4f}\\trLoss: {:4.2f}\\tTrain Acc: {:.4f}'\n",
    "        s = s.format(epoch,\n",
    "                100. * batch_idx / len(dataloader), loss.item(), running_loss, running_acc)\n",
    "        sys.stdout.write('{}\\r'.format(s))\n",
    "        sys.stdout.flush()\n",
    "    return running_loss, running_corrects, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, optimizer, epoch, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    running_inputs=0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        running_inputs += len(labels.data)\n",
    "        \n",
    "        running_acc = (1.0 * running_corrects)/running_inputs\n",
    "        \n",
    "#         s = 'Test Epoch: {:3d} ({:3.0f}%)\\tLoss:\\t{:4.4f}\\trLoss: {:4.2f}\\trPreds: {:.4f}'\n",
    "#         s = s.format(epoch,\n",
    "#                 100. * batch_idx / len(dataloader), loss.item(), running_loss, running_acc)\n",
    "#         sys.stdout.write('{}\\r'.format(s))\n",
    "#         sys.stdout.flush()\n",
    "\n",
    "    return running_loss, running_acc#, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:   1 ( 99%)\tLoss:\t1.4531\trLoss: 7712.17\tTrain Acc: 0.2780 Test Acc: 0.2669\tTime: 2.7911\n",
      "Train Epoch:   2 ( 99%)\tLoss:\t1.1340\trLoss: 6642.32\tTrain Acc: 0.3931 Test Acc: 0.5648\tTime: 2.7680\n",
      "Train Epoch:   3 ( 99%)\tLoss:\t1.1031\trLoss: 5608.32\tTrain Acc: 0.5534 Test Acc: 0.5953\tTime: 2.7717\n",
      "Train Epoch:   4 ( 99%)\tLoss:\t1.0279\trLoss: 5228.32\tTrain Acc: 0.5760 Test Acc: 0.6318\tTime: 2.8136\n",
      "Train Epoch:   5 ( 99%)\tLoss:\t1.0604\trLoss: 5103.08\tTrain Acc: 0.5889 Test Acc: 0.6508\tTime: 2.8166\n",
      "Train Epoch:   6 ( 99%)\tLoss:\t1.0457\trLoss: 5046.50\tTrain Acc: 0.5971 Test Acc: 0.6538\tTime: 2.8024\n",
      "Train Epoch:   7 ( 99%)\tLoss:\t1.3121\trLoss: 5023.44\tTrain Acc: 0.5927 Test Acc: 0.6492\tTime: 2.8264\n",
      "Train Epoch:   8 ( 99%)\tLoss:\t0.9636\trLoss: 4992.35\tTrain Acc: 0.5912 Test Acc: 0.6750\tTime: 2.7940\n",
      "Train Epoch:   9 ( 99%)\tLoss:\t1.1102\trLoss: 4890.07\tTrain Acc: 0.6050 Test Acc: 0.6712\tTime: 2.7876\n",
      "Train Epoch:  10 ( 99%)\tLoss:\t1.0373\trLoss: 4909.93\tTrain Acc: 0.6042 Test Acc: 0.6555\tTime: 2.7937\n",
      "Train Epoch:  11 ( 99%)\tLoss:\t1.1757\trLoss: 4874.05\tTrain Acc: 0.6033 Test Acc: 0.6657\tTime: 2.7898\n",
      "Train Epoch:  12 ( 99%)\tLoss:\t0.9705\trLoss: 4874.05\tTrain Acc: 0.5979 Test Acc: 0.6682\tTime: 2.7821\n",
      "Train Epoch:  13 ( 99%)\tLoss:\t1.0598\trLoss: 4810.54\tTrain Acc: 0.6073 Test Acc: 0.6610\tTime: 2.7854\n",
      "Train Epoch:  14 ( 99%)\tLoss:\t0.9109\trLoss: 4814.09\tTrain Acc: 0.6033 Test Acc: 0.6775\tTime: 2.7606\n",
      "Train Epoch:  15 ( 99%)\tLoss:\t1.0076\trLoss: 4777.32\tTrain Acc: 0.6140 Test Acc: 0.6144\tTime: 2.7810\n",
      "Train Epoch:  16 ( 99%)\tLoss:\t1.3971\trLoss: 4794.46\tTrain Acc: 0.6092 Test Acc: 0.6644\tTime: 2.8191\n",
      "Train Epoch:  17 ( 99%)\tLoss:\t1.0433\trLoss: 4734.02\tTrain Acc: 0.6184 Test Acc: 0.6581\tTime: 2.8112\n",
      "Train Epoch:  18 ( 99%)\tLoss:\t0.8370\trLoss: 4674.30\tTrain Acc: 0.6132 Test Acc: 0.6788\tTime: 2.8024\n",
      "Train Epoch:  19 ( 99%)\tLoss:\t0.8147\trLoss: 4685.09\tTrain Acc: 0.6194 Test Acc: 0.6890\tTime: 2.8244\n",
      "Train Epoch:  20 ( 99%)\tLoss:\t1.2579\trLoss: 4673.88\tTrain Acc: 0.6186 Test Acc: 0.6322\tTime: 2.8308\n",
      "Train Epoch:  21 ( 99%)\tLoss:\t0.9567\trLoss: 4646.57\tTrain Acc: 0.6228 Test Acc: 0.6487\tTime: 2.8356\n",
      "Train Epoch:  22 ( 99%)\tLoss:\t1.1736\trLoss: 4673.36\tTrain Acc: 0.6127 Test Acc: 0.6475\tTime: 2.7994\n",
      "Train Epoch:  23 ( 99%)\tLoss:\t1.0390\trLoss: 4611.26\tTrain Acc: 0.6213 Test Acc: 0.6314\tTime: 2.8239\n",
      "Train Epoch:  24 ( 99%)\tLoss:\t1.1995\trLoss: 4720.10\tTrain Acc: 0.6140 Test Acc: 0.5949\tTime: 2.7926\n",
      "Train Epoch:  25 ( 99%)\tLoss:\t1.2387\trLoss: 4642.88\tTrain Acc: 0.6192 Test Acc: 0.6686\tTime: 2.8279\n",
      "Train Epoch:  26 ( 99%)\tLoss:\t1.2928\trLoss: 4559.56\tTrain Acc: 0.6186 Test Acc: 0.6746\tTime: 2.8189\n",
      "Train Epoch:  27 ( 99%)\tLoss:\t1.0330\trLoss: 4589.14\tTrain Acc: 0.6240 Test Acc: 0.6648\tTime: 2.7825\n",
      "Train Epoch:  28 ( 99%)\tLoss:\t0.9329\trLoss: 4511.66\tTrain Acc: 0.6322 Test Acc: 0.6153\tTime: 2.7859\n",
      "Train Epoch:  29 ( 99%)\tLoss:\t0.8258\trLoss: 4484.08\tTrain Acc: 0.6290 Test Acc: 0.6449\tTime: 2.8107\n",
      "Train Epoch:  30 ( 99%)\tLoss:\t0.8039\trLoss: 4370.13\tTrain Acc: 0.6353 Test Acc: 0.6703\tTime: 2.8246\n",
      "Train Epoch:  31 ( 99%)\tLoss:\t0.9997\trLoss: 4341.07\tTrain Acc: 0.6382 Test Acc: 0.5428\tTime: 2.7960\n",
      "Train Epoch:  32 ( 99%)\tLoss:\t0.7543\trLoss: 4243.44\tTrain Acc: 0.6472 Test Acc: 0.4398\tTime: 2.7915\n",
      "Train Epoch:  33 ( 99%)\tLoss:\t0.9494\trLoss: 4188.26\tTrain Acc: 0.6508 Test Acc: 0.6593\tTime: 2.7992\n",
      "Train Epoch:  34 ( 99%)\tLoss:\t0.8140\trLoss: 4237.59\tTrain Acc: 0.6554 Test Acc: 0.5068\tTime: 2.7868\n",
      "Train Epoch:  35 ( 99%)\tLoss:\t0.8742\trLoss: 4310.57\tTrain Acc: 0.6489 Test Acc: 0.6758\tTime: 2.8008\n",
      "Train Epoch:  36 ( 99%)\tLoss:\t0.9760\trLoss: 4143.49\tTrain Acc: 0.6610 Test Acc: 0.6775\tTime: 2.8059\n",
      "Train Epoch:  37 ( 99%)\tLoss:\t0.7959\trLoss: 4112.43\tTrain Acc: 0.6627 Test Acc: 0.6034\tTime: 2.8022\n",
      "Train Epoch:  38 ( 99%)\tLoss:\t0.9379\trLoss: 4082.82\tTrain Acc: 0.6614 Test Acc: 0.5547\tTime: 2.8165\n",
      "Train Epoch:  39 ( 99%)\tLoss:\t1.1556\trLoss: 4082.79\tTrain Acc: 0.6629 Test Acc: 0.4703\tTime: 2.8718\n",
      "Train Epoch:  40 ( 99%)\tLoss:\t0.7967\trLoss: 4070.61\tTrain Acc: 0.6771 Test Acc: 0.6525\tTime: 2.9280\n",
      "Train Epoch:  41 ( 99%)\tLoss:\t0.7534\trLoss: 3973.46\tTrain Acc: 0.6788 Test Acc: 0.5436\tTime: 2.8179\n",
      "Train Epoch:  42 ( 99%)\tLoss:\t0.7603\trLoss: 3961.36\tTrain Acc: 0.6901 Test Acc: 0.4547\tTime: 2.8102\n",
      "Train Epoch:  43 ( 99%)\tLoss:\t0.9115\trLoss: 3944.08\tTrain Acc: 0.6907 Test Acc: 0.4801\tTime: 2.7982\n",
      "Train Epoch:  44 ( 99%)\tLoss:\t0.5278\trLoss: 3842.70\tTrain Acc: 0.6982 Test Acc: 0.5907\tTime: 2.7789\n",
      "Train Epoch:  45 ( 99%)\tLoss:\t0.6904\trLoss: 3841.67\tTrain Acc: 0.7011 Test Acc: 0.6275\tTime: 2.8144\n",
      "Train Epoch:  46 ( 99%)\tLoss:\t0.8368\trLoss: 3770.58\tTrain Acc: 0.7047 Test Acc: 0.4674\tTime: 2.7880\n",
      "Train Epoch:  47 ( 99%)\tLoss:\t1.1391\trLoss: 3885.46\tTrain Acc: 0.7020 Test Acc: 0.4831\tTime: 2.7637\n",
      "Train Epoch:  48 ( 99%)\tLoss:\t0.9415\trLoss: 3764.53\tTrain Acc: 0.7160 Test Acc: 0.7021\tTime: 2.7941\n",
      "Train Epoch:  49 ( 99%)\tLoss:\t0.7280\trLoss: 3717.64\tTrain Acc: 0.7187 Test Acc: 0.4585\tTime: 2.7943\n",
      "Train Epoch:  50 ( 99%)\tLoss:\t0.6753\trLoss: 3712.34\tTrain Acc: 0.7204 Test Acc: 0.5500\tTime: 2.8065\n",
      "Train Epoch:  51 ( 99%)\tLoss:\t0.7981\trLoss: 3617.94\tTrain Acc: 0.7335 Test Acc: 0.6004\tTime: 2.8082\n",
      "Train Epoch:  52 ( 99%)\tLoss:\t0.5250\trLoss: 3535.77\tTrain Acc: 0.7417 Test Acc: 0.6140\tTime: 2.8043\n",
      "Train Epoch:  53 ( 99%)\tLoss:\t0.7343\trLoss: 3566.41\tTrain Acc: 0.7377 Test Acc: 0.5318\tTime: 2.7510\n",
      "Train Epoch:  54 ( 99%)\tLoss:\t0.6182\trLoss: 3469.22\tTrain Acc: 0.7461 Test Acc: 0.4513\tTime: 2.7626\n",
      "Train Epoch:  55 ( 99%)\tLoss:\t0.8638\trLoss: 3558.88\tTrain Acc: 0.7375 Test Acc: 0.5856\tTime: 2.7853\n",
      "Train Epoch:  56 ( 99%)\tLoss:\t0.4611\trLoss: 3402.59\tTrain Acc: 0.7557 Test Acc: 0.4479\tTime: 2.8105\n",
      "Train Epoch:  57 ( 99%)\tLoss:\t0.3427\trLoss: 3391.83\tTrain Acc: 0.7546 Test Acc: 0.6322\tTime: 2.8109\n",
      "Train Epoch:  58 ( 99%)\tLoss:\t0.6562\trLoss: 3457.31\tTrain Acc: 0.7490 Test Acc: 0.6246\tTime: 2.8162\n",
      "Train Epoch:  59 ( 99%)\tLoss:\t0.6329\trLoss: 3377.54\tTrain Acc: 0.7536 Test Acc: 0.4708\tTime: 2.8162\n",
      "Train Epoch:  60 ( 99%)\tLoss:\t0.6940\trLoss: 3312.69\tTrain Acc: 0.7626 Test Acc: 0.5055\tTime: 2.7671\n",
      "Train Epoch:  61 ( 99%)\tLoss:\t0.8802\trLoss: 3336.40\tTrain Acc: 0.7645 Test Acc: 0.5093\tTime: 2.8109\n",
      "Train Epoch:  62 ( 99%)\tLoss:\t0.8007\trLoss: 3369.59\tTrain Acc: 0.7534 Test Acc: 0.6042\tTime: 2.7869\n",
      "Train Epoch:  63 ( 99%)\tLoss:\t0.7981\trLoss: 3258.23\tTrain Acc: 0.7655 Test Acc: 0.5114\tTime: 2.7771\n",
      "Train Epoch:  64 ( 99%)\tLoss:\t1.1011\trLoss: 3287.68\tTrain Acc: 0.7705 Test Acc: 0.6852\tTime: 2.8053\n",
      "Train Epoch:  65 ( 99%)\tLoss:\t0.6279\trLoss: 3259.40\tTrain Acc: 0.7689 Test Acc: 0.6564\tTime: 2.7886\n",
      "Train Epoch:  66 ( 99%)\tLoss:\t0.4153\trLoss: 3181.02\tTrain Acc: 0.7753 Test Acc: 0.5792\tTime: 2.8094\n",
      "Train Epoch:  67 ( 99%)\tLoss:\t0.5785\trLoss: 3118.86\tTrain Acc: 0.7799 Test Acc: 0.5847\tTime: 2.7990\n",
      "Train Epoch:  68 ( 99%)\tLoss:\t0.6365\trLoss: 3101.45\tTrain Acc: 0.7852 Test Acc: 0.5657\tTime: 2.7521\n",
      "Train Epoch:  69 ( 99%)\tLoss:\t0.6007\trLoss: 3107.80\tTrain Acc: 0.7841 Test Acc: 0.5699\tTime: 2.7947\n",
      "Train Epoch:  70 ( 99%)\tLoss:\t0.8015\trLoss: 3148.69\tTrain Acc: 0.7745 Test Acc: 0.4979\tTime: 2.7939\n",
      "Train Epoch:  71 ( 99%)\tLoss:\t0.4922\trLoss: 2974.08\tTrain Acc: 0.7904 Test Acc: 0.5928\tTime: 2.7904\n",
      "Train Epoch:  72 ( 99%)\tLoss:\t0.4006\trLoss: 3046.45\tTrain Acc: 0.7810 Test Acc: 0.5025\tTime: 2.8234\n",
      "Train Epoch:  73 ( 99%)\tLoss:\t0.6598\trLoss: 2996.53\tTrain Acc: 0.7929 Test Acc: 0.6352\tTime: 2.8277\n",
      "Train Epoch:  74 ( 99%)\tLoss:\t0.6040\trLoss: 2993.73\tTrain Acc: 0.7910 Test Acc: 0.5657\tTime: 2.7747\n",
      "Train Epoch:  75 ( 99%)\tLoss:\t0.7333\trLoss: 2951.94\tTrain Acc: 0.7956 Test Acc: 0.5242\tTime: 2.8182\n",
      "Train Epoch:  76 ( 99%)\tLoss:\t0.6913\trLoss: 2842.23\tTrain Acc: 0.8023 Test Acc: 0.6169\tTime: 2.7713\n",
      "Train Epoch:  77 ( 99%)\tLoss:\t0.4585\trLoss: 2872.60\tTrain Acc: 0.7973 Test Acc: 0.6364\tTime: 2.7814\n",
      "Train Epoch:  78 ( 99%)\tLoss:\t0.4621\trLoss: 2789.87\tTrain Acc: 0.8086 Test Acc: 0.5220\tTime: 2.8075\n",
      "Train Epoch:  79 ( 99%)\tLoss:\t0.5602\trLoss: 2925.93\tTrain Acc: 0.7973 Test Acc: 0.5513\tTime: 2.8169\n",
      "Train Epoch:  80 ( 99%)\tLoss:\t0.5199\trLoss: 2776.63\tTrain Acc: 0.8113 Test Acc: 0.6589\tTime: 2.7855\n",
      "Train Epoch:  81 ( 99%)\tLoss:\t0.6758\trLoss: 2754.02\tTrain Acc: 0.8109 Test Acc: 0.5415\tTime: 2.8420\n",
      "Train Epoch:  82 ( 99%)\tLoss:\t0.5853\trLoss: 2574.68\tTrain Acc: 0.8282 Test Acc: 0.6551\tTime: 2.7974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  83 ( 99%)\tLoss:\t0.4542\trLoss: 2640.46\tTrain Acc: 0.8180 Test Acc: 0.4581\tTime: 2.7853\n",
      "Train Epoch:  84 ( 99%)\tLoss:\t0.7748\trLoss: 2744.30\tTrain Acc: 0.8098 Test Acc: 0.4869\tTime: 2.7958\n",
      "Train Epoch:  85 ( 99%)\tLoss:\t0.6497\trLoss: 2693.84\tTrain Acc: 0.8199 Test Acc: 0.6614\tTime: 2.7869\n",
      "Train Epoch:  86 ( 99%)\tLoss:\t0.6035\trLoss: 2551.69\tTrain Acc: 0.8245 Test Acc: 0.5792\tTime: 2.7559\n",
      "Train Epoch:  87 ( 99%)\tLoss:\t0.4292\trLoss: 2563.79\tTrain Acc: 0.8270 Test Acc: 0.5725\tTime: 2.8135\n",
      "Train Epoch:  88 ( 99%)\tLoss:\t0.5932\trLoss: 2501.33\tTrain Acc: 0.8328 Test Acc: 0.5106\tTime: 2.7974\n",
      "Train Epoch:  89 ( 99%)\tLoss:\t0.2369\trLoss: 2539.14\tTrain Acc: 0.8272 Test Acc: 0.6441\tTime: 2.8237\n",
      "Train Epoch:  90 ( 99%)\tLoss:\t0.4423\trLoss: 2436.58\tTrain Acc: 0.8332 Test Acc: 0.6691\tTime: 2.8001\n",
      "Train Epoch:  91 ( 99%)\tLoss:\t0.4467\trLoss: 2457.30\tTrain Acc: 0.8382 Test Acc: 0.5093\tTime: 2.8080\n",
      "Train Epoch:  92 ( 99%)\tLoss:\t0.5966\trLoss: 2424.24\tTrain Acc: 0.8362 Test Acc: 0.5576\tTime: 2.8130\n",
      "Train Epoch:  93 ( 99%)\tLoss:\t0.3580\trLoss: 2404.83\tTrain Acc: 0.8328 Test Acc: 0.6814\tTime: 2.7918\n",
      "Train Epoch:  94 ( 99%)\tLoss:\t0.3858\trLoss: 2229.17\tTrain Acc: 0.8491 Test Acc: 0.6089\tTime: 2.7819\n",
      "Train Epoch:  95 ( 99%)\tLoss:\t0.4983\trLoss: 2272.86\tTrain Acc: 0.8472 Test Acc: 0.5623\tTime: 2.8017\n",
      "Train Epoch:  96 ( 99%)\tLoss:\t0.5533\trLoss: 2241.17\tTrain Acc: 0.8481 Test Acc: 0.5657\tTime: 2.7999\n",
      "Train Epoch:  97 ( 99%)\tLoss:\t0.3657\trLoss: 2157.47\tTrain Acc: 0.8522 Test Acc: 0.6636\tTime: 2.8177\n",
      "Train Epoch:  98 ( 99%)\tLoss:\t0.4928\trLoss: 2199.29\tTrain Acc: 0.8504 Test Acc: 0.4809\tTime: 2.7934\n",
      "Train Epoch:  99 ( 99%)\tLoss:\t0.3516\trLoss: 2126.31\tTrain Acc: 0.8537 Test Acc: 0.5941\tTime: 2.7582\n",
      "Train Epoch: 100 ( 99%)\tLoss:\t0.7334\trLoss: 2211.08\tTrain Acc: 0.8514 Test Acc: 0.5250\tTime: 2.7808\n"
     ]
    }
   ],
   "source": [
    "k = [16, 'M', 32, 'M', 64, 64, 'M', 128, 128, 'M']\n",
    "model = DeepClassifier(k=k, batchnorm=False).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=1E-5)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    start = time.time()\n",
    "    train_l, train_c, train_s = train(model, train_dataloader, optimizer, epoch, criterion)\n",
    "    test_l,  test_c           = test(model,  test_dataloader,  optimizer, epoch, criterion)\n",
    "    t = time.time() - start\n",
    "    print(train_s, 'Test Acc: {:.4f}\\tTime: {:.4f}'.format(test_c, t))\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:   1 (100%)\tLoss:\t1.0175\trLoss: 18539.22\tTrain Acc: 0.4754 Test Acc: 0.5508\tTime: 19.7872\n",
      "Train Epoch:   2 (100%)\tLoss:\t1.1239\trLoss: 16659.75\tTrain Acc: 0.5259 Test Acc: 0.6280\tTime: 19.7435\n",
      "Train Epoch:   3 (100%)\tLoss:\t1.5436\trLoss: 15691.66\tTrain Acc: 0.5445 Test Acc: 0.6716\tTime: 19.7382\n",
      "Train Epoch:   4 (100%)\tLoss:\t1.3230\trLoss: 14963.62\tTrain Acc: 0.5739 Test Acc: 0.7008\tTime: 19.7127\n",
      "Train Epoch:   5 (100%)\tLoss:\t0.7608\trLoss: 14002.21\tTrain Acc: 0.6114 Test Acc: 0.6581\tTime: 19.6995\n",
      "Train Epoch:   6 (100%)\tLoss:\t0.8188\trLoss: 13210.84\tTrain Acc: 0.6374 Test Acc: 0.6309\tTime: 19.6886\n",
      "Train Epoch:   7 (100%)\tLoss:\t0.8563\trLoss: 13041.09\tTrain Acc: 0.6421 Test Acc: 0.7178\tTime: 19.8489\n",
      "Train Epoch:   8 (100%)\tLoss:\t0.5412\trLoss: 12513.54\tTrain Acc: 0.6596 Test Acc: 0.7424\tTime: 20.6705\n",
      "Train Epoch:   9 (100%)\tLoss:\t0.5247\trLoss: 11904.67\tTrain Acc: 0.6869 Test Acc: 0.7449\tTime: 20.6111\n",
      "Train Epoch:  10 (100%)\tLoss:\t0.7558\trLoss: 11481.35\tTrain Acc: 0.6931 Test Acc: 0.7352\tTime: 20.6592\n",
      "Train Epoch:  11 (100%)\tLoss:\t0.4636\trLoss: 10549.09\tTrain Acc: 0.7254 Test Acc: 0.6716\tTime: 20.5933\n",
      "Train Epoch:  12 (100%)\tLoss:\t0.4426\trLoss: 10415.78\tTrain Acc: 0.7278 Test Acc: 0.7492\tTime: 20.5893\n",
      "Train Epoch:  13 (100%)\tLoss:\t0.6931\trLoss: 10161.02\tTrain Acc: 0.7342 Test Acc: 0.6966\tTime: 20.5457\n",
      "Train Epoch:  14 (100%)\tLoss:\t0.3805\trLoss: 9926.96\tTrain Acc: 0.7443 Test Acc: 0.6856\tTime: 20.5755\n",
      "Train Epoch:  15 (100%)\tLoss:\t0.5330\trLoss: 9776.39\tTrain Acc: 0.7477 Test Acc: 0.7318\tTime: 20.5263\n",
      "Train Epoch:  16 (100%)\tLoss:\t0.6424\trLoss: 9566.42\tTrain Acc: 0.7510 Test Acc: 0.7051\tTime: 20.6361\n",
      "Train Epoch:  17 (100%)\tLoss:\t0.7524\trLoss: 9596.54\tTrain Acc: 0.7550 Test Acc: 0.7398\tTime: 20.6422\n",
      "Train Epoch:  18 (100%)\tLoss:\t0.6432\trLoss: 9369.84\tTrain Acc: 0.7589 Test Acc: 0.6275\tTime: 20.6903\n",
      "Train Epoch:  19 (100%)\tLoss:\t0.7938\trLoss: 9027.57\tTrain Acc: 0.7689 Test Acc: 0.7097\tTime: 20.6361\n",
      "Train Epoch:  20 (100%)\tLoss:\t0.4016\trLoss: 8806.41\tTrain Acc: 0.7729 Test Acc: 0.7025\tTime: 20.6083\n",
      "Train Epoch:  21 (100%)\tLoss:\t0.4958\trLoss: 8375.91\tTrain Acc: 0.7873 Test Acc: 0.7254\tTime: 20.5391\n",
      "Train Epoch:  22 (100%)\tLoss:\t0.2056\trLoss: 8143.33\tTrain Acc: 0.7941 Test Acc: 0.7169\tTime: 20.5935\n",
      "Train Epoch:  23 (100%)\tLoss:\t0.3279\trLoss: 7834.17\tTrain Acc: 0.7949 Test Acc: 0.7140\tTime: 20.5822\n",
      "Train Epoch:  24 (100%)\tLoss:\t0.5570\trLoss: 7820.93\tTrain Acc: 0.8055 Test Acc: 0.7297\tTime: 20.5834\n",
      "Train Epoch:  25 (100%)\tLoss:\t0.5668\trLoss: 7654.93\tTrain Acc: 0.8069 Test Acc: 0.6983\tTime: 20.5758\n",
      "Train Epoch:  26 (100%)\tLoss:\t0.6087\trLoss: 7513.56\tTrain Acc: 0.8128 Test Acc: 0.7081\tTime: 20.6601\n",
      "Train Epoch:  27 (100%)\tLoss:\t0.7219\trLoss: 7332.35\tTrain Acc: 0.8154 Test Acc: 0.7242\tTime: 20.4971\n",
      "Train Epoch:  28 (100%)\tLoss:\t0.4475\trLoss: 7139.17\tTrain Acc: 0.8208 Test Acc: 0.7458\tTime: 20.6046\n",
      "Train Epoch:  29 (100%)\tLoss:\t0.2352\trLoss: 6914.62\tTrain Acc: 0.8245 Test Acc: 0.6907\tTime: 20.4379\n",
      "Train Epoch:  30 (100%)\tLoss:\t1.1810\trLoss: 6882.46\tTrain Acc: 0.8302 Test Acc: 0.6386\tTime: 19.0722\n",
      "Train Epoch:  31 (100%)\tLoss:\t0.9013\trLoss: 6273.85\tTrain Acc: 0.8504 Test Acc: 0.6992\tTime: 19.5344\n",
      "Train Epoch:  32 (100%)\tLoss:\t0.4261\trLoss: 6406.39\tTrain Acc: 0.8425 Test Acc: 0.7051\tTime: 19.1361\n",
      "Train Epoch:  33 (100%)\tLoss:\t0.7359\trLoss: 6071.57\tTrain Acc: 0.8584 Test Acc: 0.7343\tTime: 18.8961\n",
      "Train Epoch:  34 (100%)\tLoss:\t0.3947\trLoss: 5842.98\tTrain Acc: 0.8594 Test Acc: 0.7208\tTime: 18.7325\n",
      "Train Epoch:  35 (100%)\tLoss:\t0.2866\trLoss: 5775.71\tTrain Acc: 0.8622 Test Acc: 0.7076\tTime: 18.9897\n",
      "Train Epoch:  36 (100%)\tLoss:\t0.0846\trLoss: 5680.81\tTrain Acc: 0.8674 Test Acc: 0.7352\tTime: 19.3259\n",
      "Train Epoch:  37 (100%)\tLoss:\t0.2304\trLoss: 5326.07\tTrain Acc: 0.8754 Test Acc: 0.7165\tTime: 19.9411\n",
      "Train Epoch:  38 (100%)\tLoss:\t0.5307\trLoss: 5542.77\tTrain Acc: 0.8695 Test Acc: 0.6771\tTime: 20.3477\n",
      "Train Epoch:  39 (100%)\tLoss:\t0.4224\trLoss: 5320.73\tTrain Acc: 0.8761 Test Acc: 0.7064\tTime: 19.7742\n",
      "Train Epoch:  40 (100%)\tLoss:\t0.0862\trLoss: 5021.28\tTrain Acc: 0.8838 Test Acc: 0.6864\tTime: 19.4848\n",
      "Train Epoch:  41 (100%)\tLoss:\t0.2030\trLoss: 5099.75\tTrain Acc: 0.8773 Test Acc: 0.7169\tTime: 19.5007\n",
      "Train Epoch:  42 (100%)\tLoss:\t0.3259\trLoss: 4967.96\tTrain Acc: 0.8851 Test Acc: 0.7051\tTime: 19.5539\n",
      "Train Epoch:  43 (100%)\tLoss:\t0.2334\trLoss: 4975.93\tTrain Acc: 0.8858 Test Acc: 0.6415\tTime: 19.5779\n",
      "Train Epoch:  44 (100%)\tLoss:\t0.3162\trLoss: 4863.36\tTrain Acc: 0.8869 Test Acc: 0.6593\tTime: 19.6988\n",
      "Train Epoch:  45 (100%)\tLoss:\t0.2063\trLoss: 4511.08\tTrain Acc: 0.8947 Test Acc: 0.6801\tTime: 19.6223\n",
      "Train Epoch:  46 (100%)\tLoss:\t0.0721\trLoss: 4662.87\tTrain Acc: 0.8910 Test Acc: 0.7203\tTime: 19.6945\n",
      "Train Epoch:  47 (100%)\tLoss:\t0.6849\trLoss: 4683.06\tTrain Acc: 0.8924 Test Acc: 0.6475\tTime: 18.7987\n",
      "Train Epoch:  48 (100%)\tLoss:\t0.2324\trLoss: 4542.51\tTrain Acc: 0.8940 Test Acc: 0.6919\tTime: 19.6135\n",
      "Train Epoch:  49 (100%)\tLoss:\t0.1427\trLoss: 4528.28\tTrain Acc: 0.8936 Test Acc: 0.7322\tTime: 19.1796\n",
      "Train Epoch:  50 (100%)\tLoss:\t0.1405\trLoss: 4634.23\tTrain Acc: 0.8918 Test Acc: 0.6665\tTime: 18.8729\n",
      "Train Epoch:  51 (100%)\tLoss:\t0.1181\trLoss: 4238.91\tTrain Acc: 0.9021 Test Acc: 0.6076\tTime: 19.3240\n",
      "Train Epoch:  52 (100%)\tLoss:\t0.2966\trLoss: 4308.43\tTrain Acc: 0.8999 Test Acc: 0.6153\tTime: 19.6408\n",
      "Train Epoch:  53 (100%)\tLoss:\t0.3555\trLoss: 4359.97\tTrain Acc: 0.8962 Test Acc: 0.5886\tTime: 19.6312\n",
      "Train Epoch:  54 (100%)\tLoss:\t0.2700\trLoss: 4275.19\tTrain Acc: 0.9046 Test Acc: 0.6636\tTime: 19.5325\n",
      "Train Epoch:  55 (100%)\tLoss:\t0.3532\trLoss: 4305.02\tTrain Acc: 0.9012 Test Acc: 0.6856\tTime: 19.7987\n",
      "Train Epoch:  56 (100%)\tLoss:\t0.0845\trLoss: 4004.68\tTrain Acc: 0.9100 Test Acc: 0.6839\tTime: 19.8968\n",
      "Train Epoch:  57 (100%)\tLoss:\t0.2433\trLoss: 3956.25\tTrain Acc: 0.9090 Test Acc: 0.6631\tTime: 19.4824\n",
      "Train Epoch:  58 (100%)\tLoss:\t0.2454\trLoss: 3893.08\tTrain Acc: 0.9131 Test Acc: 0.6758\tTime: 20.1842\n",
      "Train Epoch:  59 (100%)\tLoss:\t0.3508\trLoss: 4019.64\tTrain Acc: 0.9087 Test Acc: 0.6928\tTime: 19.6253\n",
      "Train Epoch:  60 (100%)\tLoss:\t0.0982\trLoss: 4037.73\tTrain Acc: 0.9071 Test Acc: 0.6174\tTime: 19.2827\n",
      "Train Epoch:  61 (100%)\tLoss:\t0.1436\trLoss: 3907.55\tTrain Acc: 0.9101 Test Acc: 0.7051\tTime: 19.6053\n",
      "Train Epoch:  62 (100%)\tLoss:\t0.1915\trLoss: 3972.91\tTrain Acc: 0.9099 Test Acc: 0.6453\tTime: 19.5296\n",
      "Train Epoch:  63 (100%)\tLoss:\t0.1767\trLoss: 3744.53\tTrain Acc: 0.9143 Test Acc: 0.6449\tTime: 19.5572\n",
      "Train Epoch:  64 (100%)\tLoss:\t0.1224\trLoss: 3653.43\tTrain Acc: 0.9177 Test Acc: 0.6403\tTime: 19.5918\n",
      "Train Epoch:  65 (100%)\tLoss:\t0.5597\trLoss: 3838.87\tTrain Acc: 0.9119 Test Acc: 0.6911\tTime: 19.6483\n",
      "Train Epoch:  66 (100%)\tLoss:\t0.2135\trLoss: 3863.59\tTrain Acc: 0.9135 Test Acc: 0.6835\tTime: 19.4528\n",
      "Train Epoch:  67 (100%)\tLoss:\t0.5023\trLoss: 3521.36\tTrain Acc: 0.9191 Test Acc: 0.6534\tTime: 19.4642\n",
      "Train Epoch:  68 (100%)\tLoss:\t0.1843\trLoss: 3826.43\tTrain Acc: 0.9129 Test Acc: 0.6161\tTime: 19.5894\n",
      "Train Epoch:  69 (100%)\tLoss:\t0.1774\trLoss: 3738.63\tTrain Acc: 0.9153 Test Acc: 0.5415\tTime: 19.5815\n",
      "Train Epoch:  70 (100%)\tLoss:\t0.1353\trLoss: 3583.82\tTrain Acc: 0.9170 Test Acc: 0.6703\tTime: 19.5713\n",
      "Train Epoch:  71 (100%)\tLoss:\t0.4954\trLoss: 3630.64\tTrain Acc: 0.9184 Test Acc: 0.6208\tTime: 19.5288\n",
      "Train Epoch:  72 (100%)\tLoss:\t0.3489\trLoss: 3591.71\tTrain Acc: 0.9163 Test Acc: 0.6669\tTime: 19.5979\n",
      "Train Epoch:  73 (100%)\tLoss:\t0.3316\trLoss: 3492.50\tTrain Acc: 0.9218 Test Acc: 0.6326\tTime: 19.5761\n",
      "Train Epoch:  74 (100%)\tLoss:\t0.6381\trLoss: 3658.78\tTrain Acc: 0.9169 Test Acc: 0.6508\tTime: 19.5879\n",
      "Train Epoch:  75 (100%)\tLoss:\t0.1392\trLoss: 3506.42\tTrain Acc: 0.9186 Test Acc: 0.6208\tTime: 19.6266\n",
      "Train Epoch:  76 (100%)\tLoss:\t0.2134\trLoss: 3288.28\tTrain Acc: 0.9233 Test Acc: 0.5780\tTime: 19.6103\n",
      "Train Epoch:  77 (100%)\tLoss:\t0.1890\trLoss: 3312.55\tTrain Acc: 0.9213 Test Acc: 0.6708\tTime: 19.5459\n",
      "Train Epoch:  78 (100%)\tLoss:\t0.1724\trLoss: 3184.72\tTrain Acc: 0.9253 Test Acc: 0.6530\tTime: 19.5257\n",
      "Train Epoch:  79 (100%)\tLoss:\t0.1021\trLoss: 3193.28\tTrain Acc: 0.9220 Test Acc: 0.5970\tTime: 19.4582\n",
      "Train Epoch:  80 (100%)\tLoss:\t0.3574\trLoss: 3151.34\tTrain Acc: 0.9243 Test Acc: 0.6928\tTime: 19.5535\n",
      "Train Epoch:  81 (100%)\tLoss:\t0.3218\trLoss: 3280.61\tTrain Acc: 0.9200 Test Acc: 0.6208\tTime: 19.4934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  82 (100%)\tLoss:\t0.3703\trLoss: 3100.65\tTrain Acc: 0.9256 Test Acc: 0.5742\tTime: 19.5800\n",
      "Train Epoch:  83 (100%)\tLoss:\t0.2236\trLoss: 3058.07\tTrain Acc: 0.9234 Test Acc: 0.5500\tTime: 19.7094\n",
      "Train Epoch:  84 (100%)\tLoss:\t0.1332\trLoss: 2991.91\tTrain Acc: 0.9280 Test Acc: 0.6250\tTime: 19.7085\n",
      "Train Epoch:  85 (100%)\tLoss:\t0.0527\trLoss: 2946.16\tTrain Acc: 0.9299 Test Acc: 0.6225\tTime: 19.5057\n",
      "Train Epoch:  86 (100%)\tLoss:\t0.3163\trLoss: 2810.35\tTrain Acc: 0.9305 Test Acc: 0.6619\tTime: 19.0402\n",
      "Train Epoch:  87 (100%)\tLoss:\t0.0327\trLoss: 2945.06\tTrain Acc: 0.9289 Test Acc: 0.5458\tTime: 19.7019\n",
      "Train Epoch:  88 (100%)\tLoss:\t0.2243\trLoss: 2680.63\tTrain Acc: 0.9353 Test Acc: 0.5364\tTime: 19.1392\n",
      "Train Epoch:  89 (100%)\tLoss:\t0.4447\trLoss: 2701.02\tTrain Acc: 0.9339 Test Acc: 0.5377\tTime: 19.1494\n",
      "Train Epoch:  90 (100%)\tLoss:\t0.1578\trLoss: 3120.55\tTrain Acc: 0.9259 Test Acc: 0.6915\tTime: 19.9723\n",
      "Train Epoch:  91 (100%)\tLoss:\t0.0553\trLoss: 2783.22\tTrain Acc: 0.9334 Test Acc: 0.5059\tTime: 20.6671\n",
      "Train Epoch:  92 (100%)\tLoss:\t0.0588\trLoss: 2703.46\tTrain Acc: 0.9334 Test Acc: 0.6165\tTime: 20.3969\n",
      "Train Epoch:  93 (100%)\tLoss:\t0.5819\trLoss: 2692.04\tTrain Acc: 0.9338 Test Acc: 0.6898\tTime: 20.2759\n",
      "Train Epoch:  94 (100%)\tLoss:\t0.0175\trLoss: 2711.64\tTrain Acc: 0.9335 Test Acc: 0.5746\tTime: 20.7951\n",
      "Train Epoch:  95 (100%)\tLoss:\t0.0695\trLoss: 2862.33\tTrain Acc: 0.9296 Test Acc: 0.6682\tTime: 20.6893\n",
      "Train Epoch:  96 (100%)\tLoss:\t0.2285\trLoss: 2869.99\tTrain Acc: 0.9294 Test Acc: 0.5809\tTime: 20.4566\n",
      "Train Epoch:  97 ( 78%)\tLoss:\t0.1399\trLoss: 2105.47\tTrain Acc: 0.9342\r"
     ]
    }
   ],
   "source": [
    "model = DeepClassifier(k=cfgs['vgg11'], batchnorm=False).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=1E-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    start = time.time()\n",
    "    train_l, train_c, train_s = train(model, train_dataloader, optimizer, epoch, criterion)\n",
    "    test_l,  test_c           = test(model,  test_dataloader,  optimizer, epoch, criterion)\n",
    "    t = time.time() - start\n",
    "    print(train_s, 'Test Acc: {:.4f}\\tTime: {:.4f}'.format(test_c, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
